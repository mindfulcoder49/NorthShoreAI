{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of the Jupyter Notebook\n",
        "\n",
        "### Key Components:\n",
        "- **Libraries Installation**: The notebook begins by installing necessary Python libraries such as `gradio`, `openai`, and `tiktoken` which are used for creating web interfaces, interacting with AI models, and token analysis, respectively.\n",
        "- **Function Definitions**: Several Python functions are defined to perform tasks such as database backup, restore operations, and interaction logging. These functions help in managing data effectively.\n",
        "- **Database Management**: Uses SQLite database for storing and managing interaction logs and templates.\n",
        "- **Gradio Interface**: A web-based interface is set up using Gradio, enabling users to interact with the notebook through a web page, making it user-friendly for non-programmers.\n",
        "\n",
        "### Setting Up Google Drive and OpenAI API in Colab:\n",
        "1. **Google Drive Integration**:\n",
        "   - **Purpose**: To store backups of databases and CSV files.\n",
        "   - **Steps**:\n",
        "     1. Mount Google Drive in the notebook: Click the folder icon to the left underneath the key icon. Then click the folder icon at the top with the drive icon.\n",
        "\n",
        "     2. The mounted drive acts as a directory in Google Colab, allowing the notebook to read from and write to your Google Drive.\n",
        "\n",
        "2. **OpenAI API Setup**:\n",
        "   - **Purpose**: To use models like GPT-3.5 or GPT-4 for generating text based on the input provided.\n",
        "   - **Steps**:\n",
        "     1. Create an OpenAI account and generate an API key from [platform.openai.com](https://platform.openai.com).\n",
        "     2. Add the key to the Secrets in Colab (see detailed unstructions below)\n",
        "     \n",
        "\n"
      ],
      "metadata": {
        "id": "x-gC4C85h0bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkM0jSQgV4sn",
        "outputId": "03b320d4-fff6-4b4c-d699-42af377bfd15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Data Flows In This Tool\n",
        "\n",
        "The Jupyter notebook you're using integrates multiple data flows that connect the temporary Google Colab runtime environment with the OpenAI API and the user's permanent Google Drive storage. Hereâ€™s a summary of these data flows:\n",
        "\n",
        "### 1. **Data Flow Between Google Colab and Google Drive:**\n",
        "- **Manual Backup Operations**: Data from the SQLite database stored temporarily in the Colab runtime can be backed up to Google Drive. This includes copying database files (`.db`, `.db-shm`, `.db-wal`) and converting database tables to CSV format for storage.\n",
        "- **Restore Operations**: Data can be restored from Google Drive back to the Google Colab environment. This involves copying the backup files from Drive to the Colab runtime.\n",
        "- **Automatic Backups**: The tool can be configured to automatically back up the database files to Google Drive after every change, ensuring data persistence beyond the temporary session. This uses specific autobackup files and does not affect manual backups.\n",
        "\n",
        "### 2. **Data Flow Between Google Colab and OpenAI API:**\n",
        "- **Text Generation**: The tool sends text inputs to the OpenAI API to generate responses based on the model selected (e.g., GPT-3.5 or GPT-4).\n",
        "- **Cost Estimation**: Before sending data to the OpenAI API, the tool can calculate the number of tokens and the associated cost.\n",
        "\n",
        "### 3. **Internal Data Flow Within Google Colab:**\n",
        "- **Data Handling and Processing**: Data is handled internally within the Colab notebook in SQLite databases within the Colab environment.\n",
        "- **Interaction Logging**: Interactions through the Gradio interface are logged in an SQLite database for record-keeping and future reference. This includes details of the interactions and the generated responses.\n",
        "\n",
        "### 4. **Combined Data Flows for Enhanced Functionality:**\n",
        "- **Template Management**: Templates for text interactions can be saved to and loaded from the SQLite database, allowing users to reuse and modify predefined text patterns for consistent interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZA7fFJsLtby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to add your OPENAI API key to this notebook\n",
        "\n",
        "1. You will have to create an OpenAI account and generate an API key from platform.openai.com.\n",
        "\n",
        "2. Then, setup your OPENAI_API_KEY under \"Secrets\"\n",
        "\n",
        "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "\n",
        "3. Add your API key to the Colab Secrets manager to securely store it.\n",
        "\n",
        "4. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel.\n",
        "\n",
        "5. The Secrets tab is found on the left panel.\n",
        "Create a new secret with the name OPENAI_API_KEY.\n",
        "\n",
        "6. Copy/paste your API key into the Value input box of OPENAI_API_KEY.\n",
        "\n",
        "7. Toggle the button on the left to allow notebook access to the secret."
      ],
      "metadata": {
        "id": "wGOf23TuM5kE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpLyDYh89Bo",
        "outputId": "422f01da-b158-4eb3-b715-1ac89ad354e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q dataset ipywidgets gradio openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8lVNwkpGpal4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import dataset\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import tiktoken\n",
        "import os\n",
        "import shutil\n",
        "import csv\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup global database variable\n",
        "db = dataset.connect('sqlite:///interactions.db')"
      ],
      "metadata": {
        "id": "Gd6leOL4SwI_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Aawbnb-g8wHz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def backup_db_files(source_dir, destination_dir):\n",
        "    messages = []\n",
        "\n",
        "    if not os.path.exists(source_dir):\n",
        "        messages.append(\"Source directory does not exist.\")\n",
        "        return messages\n",
        "\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "        messages.append(f\"Destination directory created: {destination_dir}\")\n",
        "\n",
        "    db_extensions = ('.db', '.db-shm', '.db-wal')\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")  # Format datetime\n",
        "\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith(db_extensions):\n",
        "            source_file = os.path.join(source_dir, filename)\n",
        "            modified_filename = f\"{os.path.splitext(filename)[0]}_{current_time}{os.path.splitext(filename)[1]}\"\n",
        "            destination_file = os.path.join(destination_dir, modified_filename)\n",
        "            shutil.copy2(source_file, destination_file)\n",
        "            messages.append(f\"Copied {filename} to {destination_file}\")\n",
        "\n",
        "    return messages\n",
        "\n",
        "def backup_db_to_drive_as_csv(source_dir, destination_dir):\n",
        "    messages = []\n",
        "\n",
        "    if not os.path.exists(source_dir):\n",
        "        messages.append(\"Source directory does not exist.\")\n",
        "        return messages\n",
        "\n",
        "    db_extensions = ('.db', '.db-shm', '.db-wal')\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")  # Format datetime\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith('.db'):  # Only process .db files for CSV conversion\n",
        "            source_file = os.path.join(source_dir, filename)\n",
        "            # Connect to the SQLite database\n",
        "            conn = sqlite3.connect(source_file)\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "            tables = cursor.fetchall()\n",
        "\n",
        "            for table_name in tables:\n",
        "                table_name = table_name[0]\n",
        "                query = f\"SELECT * FROM {table_name}\"\n",
        "                data = cursor.execute(query)\n",
        "\n",
        "                csv_filename = f\"{os.path.splitext(filename)[0]}_{table_name}_{current_time}.csv\"\n",
        "                csv_file_path = os.path.join(source_dir, csv_filename)\n",
        "\n",
        "                with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "                    csv_writer = csv.writer(csv_file)\n",
        "                    # Write headers\n",
        "                    csv_writer.writerow([i[0] for i in cursor.description])\n",
        "                    # Write data\n",
        "                    csv_writer.writerows(data)\n",
        "\n",
        "                messages.append(f\"CSV file created in source directory: {csv_filename}\")\n",
        "\n",
        "                # If destination directory exists, copy the CSV file there\n",
        "                if os.path.exists(destination_dir):\n",
        "                    destination_csv_file_path = os.path.join(destination_dir, csv_filename)\n",
        "                    shutil.copy2(csv_file_path, destination_csv_file_path)\n",
        "                    messages.append(f\"Copied {csv_filename} to {destination_dir}\")\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "def autobackup_db_files(source_dir, destination_dir):\n",
        "    messages = []\n",
        "\n",
        "    if not os.path.exists(source_dir):\n",
        "        messages.append(\"Source directory does not exist.\")\n",
        "        return messages\n",
        "\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "        messages.append(f\"Destination directory created: {destination_dir}\")\n",
        "\n",
        "    db_extensions = ('.db', '.db-shm', '.db-wal')\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")  # Format datetime\n",
        "\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith(db_extensions):\n",
        "            source_file = os.path.join(source_dir, filename)\n",
        "            modified_filename = f\"{os.path.splitext(filename)[0]}_autobackup{os.path.splitext(filename)[1]}\"\n",
        "            destination_file = os.path.join(destination_dir, modified_filename)\n",
        "            shutil.copy2(source_file, destination_file)\n",
        "            messages.append(f\"Copied {filename} to {destination_file}\")\n",
        "\n",
        "    return messages\n",
        "\n",
        "#create function for restoring the autobackup files\n",
        "def restore_auto_backup(backup_dir, restore_dir, base_filename):\n",
        "    messages = []\n",
        "\n",
        "    if not os.path.exists(restore_dir):\n",
        "        os.makedirs(restore_dir)\n",
        "        messages.append(f\"Restore directory created: {restore_dir}\")\n",
        "\n",
        "    autobackup_files = [\"interactions_autobackup.db\", \"interactions_autobackup.db-shm\", \"interactions_autobackup.db-wal\"]\n",
        "\n",
        "    #copy the autobackup files to the restore directory but remove the _autobackup\n",
        "    for filename in autobackup_files:\n",
        "        source_file = os.path.join(backup_dir, filename)\n",
        "        modified_filename = filename.replace(\"_autobackup\", \"\")\n",
        "        restore_file = os.path.join(restore_dir, modified_filename)\n",
        "        shutil.copy2(source_file, restore_file)\n",
        "        messages.append(f\"Copied {filename} to {restore_file}\")\n",
        "\n",
        "    template_names = get_template_names()\n",
        "\n",
        "    return messages, gr.Dropdown(label=\"Select Template\", choices=template_names, value=template_names[0] if template_names else None)\n",
        "\n",
        "\n",
        "def restore_db_files(backup_dir, restore_dir, base_filename):\n",
        "    messages = []\n",
        "\n",
        "    if not os.path.exists(restore_dir):\n",
        "        os.makedirs(restore_dir)\n",
        "        messages.append(f\"Restore directory created: {restore_dir}\")\n",
        "\n",
        "    latest_common_timestamp = None\n",
        "    candidate_files = {'.db': None, '.db-shm': None, '.db-wal': None}\n",
        "\n",
        "    # Gather all files and their timestamps\n",
        "    file_timestamps = {}\n",
        "    for filename in os.listdir(backup_dir):\n",
        "        if filename.startswith(base_filename) and any(filename.endswith(ext) for ext in ['.db', '.db-shm', '.db-wal']):\n",
        "            match = re.search(r\"_(\\d{14})\\.\", filename)\n",
        "            if match:\n",
        "                timestamp = match.group(1)\n",
        "                ext = os.path.splitext(filename)[1]\n",
        "                if timestamp not in file_timestamps:\n",
        "                    file_timestamps[timestamp] = {}\n",
        "                file_timestamps[timestamp][ext] = filename\n",
        "\n",
        "    # Determine the latest timestamp where all file types are present\n",
        "    for timestamp, files in file_timestamps.items():\n",
        "        if all(ext in files for ext in ['.db', '.db-shm', '.db-wal']):\n",
        "            if not latest_common_timestamp or timestamp > latest_common_timestamp:\n",
        "                latest_common_timestamp = timestamp\n",
        "                candidate_files = files\n",
        "\n",
        "    if latest_common_timestamp:\n",
        "        # Restore the files with the latest common timestamp\n",
        "        for ext, filename in candidate_files.items():\n",
        "            source_file = os.path.join(backup_dir, filename)\n",
        "            restore_file = os.path.join(restore_dir, f\"{base_filename}{ext}\")\n",
        "            shutil.copy2(source_file, restore_file)\n",
        "            messages.append(f\"Restored {filename} to {restore_file}\")\n",
        "    else:\n",
        "        messages.append(\"No complete set of backup files found with a common timestamp.\")\n",
        "\n",
        "    template_names = get_template_names()\n",
        "\n",
        "    return messages, gr.Dropdown(label=\"Select Template\", choices=template_names, value=template_names[0] if template_names else None)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r6OYVCZ2YPFP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Instantiate the OpenAI client\n",
        "def generate_ai_response(model, temperature, max_tokens, user_input):\n",
        "    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "def setup_database():\n",
        "    global db\n",
        "    if 'interactions' not in db.tables:\n",
        "        db.create_table('interactions')\n",
        "        db['interactions'].create_column('timestamp', db.types.string)\n",
        "        db['interactions'].create_column('introduction', db.types.string)\n",
        "        db['interactions'].create_column('field_1_intro', db.types.string)\n",
        "        db['interactions'].create_column('field_1_name', db.types.string)\n",
        "        db['interactions'].create_column('field_1_content', db.types.string)\n",
        "        db['interactions'].create_column('field_2_intro', db.types.string)\n",
        "        db['interactions'].create_column('field_2_name', db.types.string)\n",
        "        db['interactions'].create_column('field_2_content', db.types.string)\n",
        "        db['interactions'].create_column('field_3_intro', db.types.string)\n",
        "        db['interactions'].create_column('field_3_name', db.types.string)\n",
        "        db['interactions'].create_column('field_3_content', db.types.string)\n",
        "        db['interactions'].create_column('field_4_intro', db.types.string)\n",
        "        db['interactions'].create_column('field_4_name', db.types.string)\n",
        "        db['interactions'].create_column('field_4_content', db.types.string)\n",
        "        db['interactions'].create_column('conclusion', db.types.string)\n",
        "        db['interactions'].create_column('generated_text', db.types.string)\n",
        "    if 'templates' not in db.tables:\n",
        "        db.create_table('templates')\n",
        "        db['templates'].create_column('template_name', db.types.string)\n",
        "        db['templates'].create_column('introduction', db.types.string)\n",
        "        db['templates'].create_column('field_1_intro', db.types.string)\n",
        "        db['templates'].create_column('field_1_name', db.types.string)\n",
        "        db['templates'].create_column('field_1_content', db.types.string)\n",
        "        db['templates'].create_column('field_2_intro', db.types.string)\n",
        "        db['templates'].create_column('field_2_name', db.types.string)\n",
        "        db['templates'].create_column('field_2_content', db.types.string)\n",
        "        db['templates'].create_column('field_3_intro', db.types.string)\n",
        "        db['templates'].create_column('field_3_name', db.types.string)\n",
        "        db['templates'].create_column('field_3_content', db.types.string)\n",
        "        db['templates'].create_column('field_4_intro', db.types.string)\n",
        "        db['templates'].create_column('field_4_name', db.types.string)\n",
        "        db['templates'].create_column('field_4_content', db.types.string)\n",
        "        db['templates'].create_column('conclusion', db.types.string)\n",
        "\n",
        "        default_template_exists = db['templates'].find_one(template_name='default')\n",
        "        if not default_template_exists:\n",
        "            db['templates'].insert({\n",
        "                'template_name': 'default',\n",
        "                'introduction': '',\n",
        "                'field_1_intro': '',\n",
        "                'field_1_name': '',\n",
        "                'field_1_content': '',\n",
        "                'field_2_intro': '',\n",
        "                'field_2_name': '',\n",
        "                'field_2_content': '',\n",
        "                'field_3_intro': '',\n",
        "                'field_3_name': '',\n",
        "                'field_3_content': '',\n",
        "                'field_4_intro': '',\n",
        "                'field_4_name': '',\n",
        "                'field_4_content': '',\n",
        "                'conclusion': ''\n",
        "            })\n",
        "\n",
        "def log_interaction(introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion, generated_text):\n",
        "    global db\n",
        "    table = db['interactions']\n",
        "    table.insert({\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        'introduction': introduction,\n",
        "        'field_1_intro': field_1_intro,\n",
        "        'field_1_name': field_1_name,\n",
        "        'field_1_content': field_1_content,\n",
        "        'field_2_intro': field_2_intro,\n",
        "        'field_2_name': field_2_name,\n",
        "        'field_2_content': field_2_content,\n",
        "        'field_3_intro': field_3_intro,\n",
        "        'field_3_name': field_3_name,\n",
        "        'field_3_content': field_3_content,\n",
        "        'field_4_intro': field_4_intro,\n",
        "        'field_4_name': field_4_name,\n",
        "        'field_4_content': field_4_content,\n",
        "        'conclusion': conclusion,\n",
        "        'generated_text': generated_text\n",
        "    })\n",
        "\n",
        "def save_template(template_name, introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion):\n",
        "    global db\n",
        "    table = db['templates']\n",
        "    table.insert({\n",
        "        'template_name': template_name,\n",
        "        'introduction': introduction,\n",
        "        'field_1_intro': field_1_intro,\n",
        "        'field_1_name': field_1_name,\n",
        "        'field_1_content': field_1_content,\n",
        "        'field_2_intro': field_2_intro,\n",
        "        'field_2_name': field_2_name,\n",
        "        'field_2_content': field_2_content,\n",
        "        'field_3_intro': field_3_intro,\n",
        "        'field_3_name': field_3_name,\n",
        "        'field_3_content': field_3_content,\n",
        "        'field_4_intro': field_4_intro,\n",
        "        'field_4_name': field_4_name,\n",
        "        'field_4_content': field_4_content,\n",
        "        'conclusion': conclusion\n",
        "    })\n",
        "\n",
        "def read_interactions():\n",
        "    global db\n",
        "    interactions = db['interactions']\n",
        "    interaction_ids = \"\"\n",
        "\n",
        "    for interaction in interactions.all(order_by='-timestamp'):\n",
        "        #add interaction timestamp and all of the field names\n",
        "        interaction_ids += (f\"{interaction['timestamp']} - {interaction['field_1_name']} - {interaction['field_2_name']} - {interaction['field_3_name']} - {interaction['field_4_name']}\\n\")\n",
        "\n",
        "    return interaction_ids\n",
        "\n",
        "def load_interaction_details(timestamp):\n",
        "    global db\n",
        "    #trim timestamp\n",
        "    timestamp = timestamp.strip()\n",
        "    interaction = db['interactions'].find_one(timestamp=timestamp)\n",
        "    if interaction:\n",
        "        return (\n",
        "            interaction['introduction'], interaction['field_1_intro'], interaction['field_1_name'], interaction['field_1_content'],\n",
        "            interaction['field_2_intro'], interaction['field_2_name'], interaction['field_2_content'],\n",
        "            interaction['field_3_intro'], interaction['field_3_name'], interaction['field_3_content'],\n",
        "            interaction['field_4_intro'], interaction['field_4_name'], interaction['field_4_content'],\n",
        "            interaction['conclusion'], interaction['generated_text']\n",
        "        )\n",
        "    else:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" , \"\")\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def calculate_tokens_and_cost(model, user_input, encoding_name, max_tokens):\n",
        "    num_tokens = num_tokens_from_string(user_input, \"cl100k_base\")\n",
        "\n",
        "    # Model pricing structure\n",
        "    pricing = {\n",
        "        'gpt-4o': {'input': 5, 'output': 15},\n",
        "        'gpt-4-turbo': {'input': 10, 'output': 30},\n",
        "        'gpt-3.5-turbo': {'input': 0.50, 'output': 1.50}\n",
        "    }\n",
        "\n",
        "    # Calculate input and output token costs based on the model's pricing\n",
        "    input_cost = (num_tokens / 1_000_000) * pricing[model]['input']\n",
        "    output_cost = (max_tokens / 1_000_000) * pricing[model]['output']\n",
        "\n",
        "    # Total cost calculation\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    return num_tokens, total_cost\n",
        "\n",
        "def combine_fields_and_estimate_costs(model, temperature, max_tokens, introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion):\n",
        "\n",
        "    consolidated_info = (\n",
        "        f\"Introduction: {introduction}\\n\"\n",
        "        f\"{field_1_intro}: {field_1_name} - {field_1_content}\\n\"\n",
        "        f\"{field_2_intro}: {field_2_name} - {field_2_content}\\n\"\n",
        "        f\"{field_3_intro}: {field_3_name} - {field_3_content}\\n\"\n",
        "        f\"{field_4_intro}: {field_4_name} - {field_4_content}\\n\"\n",
        "        f\"Conclusion: {conclusion}\"\n",
        "    )\n",
        "\n",
        "    num_tokens, total_cost = calculate_tokens_and_cost(model, consolidated_info, \"cl100k_base\", max_tokens)\n",
        "\n",
        "    message = f\"Number of Tokens: {num_tokens}\\nTotal Cost: ${total_cost:.2f}\"\n",
        "    return message\n",
        "\n",
        "def log_and_display_interactions(autobackup_checkbox, template_name, model, temperature, max_tokens, introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion):\n",
        "    setup_database()\n",
        "    consolidated_info = (\n",
        "        f\"Introduction: {introduction}\\n\"\n",
        "        f\"{field_1_intro}: {field_1_name} - {field_1_content}\\n\"\n",
        "        f\"{field_2_intro}: {field_2_name} - {field_2_content}\\n\"\n",
        "        f\"{field_3_intro}: {field_3_name} - {field_3_content}\\n\"\n",
        "        f\"{field_4_intro}: {field_4_name} - {field_4_content}\\n\"\n",
        "        f\"Conclusion: {conclusion}\"\n",
        "    )\n",
        "    generated_text = generate_ai_response(model, temperature, max_tokens, consolidated_info)\n",
        "\n",
        "    log_interaction(introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion, generated_text)\n",
        "\n",
        "    logged_interactions = read_interactions()\n",
        "\n",
        "    if autobackup_checkbox:\n",
        "        autobackup_db_files('/content/', '/content/drive/MyDrive/InteractionTrackerBackups/')\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def save_template_handler(autobackup_checkbox, template_name, introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion):\n",
        "    if not template_name:\n",
        "        template_name = f\"{field_1_name}_{field_2_name}_{field_3_name}_{field_4_name}\"\n",
        "    save_template(template_name, introduction, field_1_intro, field_1_name, field_1_content, field_2_intro, field_2_name, field_2_content, field_3_intro, field_3_name, field_3_content, field_4_intro, field_4_name, field_4_content, conclusion)\n",
        "    template_names = get_template_names()\n",
        "\n",
        "    if autobackup_checkbox:\n",
        "        autobackup_db_files('/content/', '/content/drive/MyDrive/InteractionTrackerBackups/')\n",
        "\n",
        "    return f\"Template '{template_name}' saved successfully.\", gr.Dropdown(label=\"Select Template\", choices=template_names, value=template_name)\n",
        "\n",
        "def get_template_names():\n",
        "    global db\n",
        "    templates = db['templates']\n",
        "    return [template['template_name'] for template in templates.all()]\n",
        "\n",
        "def load_template_details(template_name):\n",
        "    global db\n",
        "    template = db['templates'].find_one(template_name=template_name)\n",
        "    if template:\n",
        "        return (template['introduction'], template['field_1_intro'], template['field_1_name'], template['field_1_content'],\n",
        "                template['field_2_intro'], template['field_2_name'], template['field_2_content'],\n",
        "                template['field_3_intro'], template['field_3_name'], template['field_3_content'],\n",
        "                template['field_4_intro'], template['field_4_name'], template['field_4_content'],\n",
        "                template['conclusion'])\n",
        "    else:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial database setup\n",
        "setup_database()\n",
        "\n",
        "#Used in Gradio interface below\n",
        "openai_model_list = ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "metadata": {
        "id": "V7Q67mt0STWm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Gradio interface components\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"Templates\"):\n",
        "        with gr.Row():\n",
        "            template_selector = gr.Dropdown(label=\"Select Template\", choices=get_template_names(), value=get_template_names()[0] if get_template_names() else None, allow_custom_value=True)\n",
        "            load_template_button = gr.Button(\"Load Template\", variant=\"primary\")\n",
        "            save_template_button = gr.Button(\"Save Template\", variant=\"secondary\")\n",
        "            estimate_costs_button = gr.Button(\"Estimate Costs\", variant=\"primary\")\n",
        "        with gr.Row():\n",
        "            template_name_input = gr.Textbox(placeholder='Enter template name (optional)', label='Template Name', scale=2)\n",
        "            model_input = gr.Dropdown(label='Model', choices=openai_model_list, value=openai_model_list[0])\n",
        "            temperature_input = gr.Slider(minimum=0, maximum=1, value=0.5, label='Temperature', scale=1)\n",
        "            max_tokens_input = gr.Slider(minimum=10, maximum=10000, value=500, label='Max Tokens', scale=2)\n",
        "        with gr.Row():\n",
        "            backup_button = gr.Button(\"Backup Database\", variant=\"secondary\", scale=1)\n",
        "            restore_button = gr.Button(\"Restore Database\", variant=\"secondary\", scale=1)\n",
        "            autobackup_checkbox = gr.Checkbox(label='Autobackup', value=True)\n",
        "            restore_autobackup_button = gr.Button(\"Restore Autobackup\", variant=\"secondary\", scale=1)\n",
        "        with gr.Row():\n",
        "            log_interaction_button = gr.Button(\"Log Interaction\", variant=\"primary\")\n",
        "            clear_button = gr.Button(\"Clear Fields\", variant=\"stop\")\n",
        "            export_csv_button = gr.Button(\"Export CSV\", variant=\"primary\")\n",
        "\n",
        "        #add status_area for return messages from buttons\n",
        "        with gr.Row():\n",
        "            status_area = gr.Textbox(placeholder='View status messages here', label='Status Message', lines=3, scale=2)\n",
        "        with gr.Row():\n",
        "            introduction_input = gr.Textbox(placeholder='Type the introduction...', label='Introduction', scale=1)\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                field_1_intro_input = gr.Textbox(placeholder='Enter first field introduction...', label='First Field Introduction')\n",
        "                field_1_name_input = gr.Textbox(placeholder='Enter first field name...', label='First Field Name')\n",
        "                field_1_content_input = gr.Textbox(placeholder='Enter first field content...', label='First Field Content')\n",
        "            with gr.Column():\n",
        "                field_2_intro_input = gr.Textbox(placeholder='Enter second field introduction...', label='Second Field Introduction')\n",
        "                field_2_name_input = gr.Textbox(placeholder='Enter second field name...', label='Second Field Name')\n",
        "                field_2_content_input = gr.Textbox(placeholder='Enter second field content...', label='Second Field Content')\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                field_3_intro_input = gr.Textbox(placeholder='Enter third field introduction...', label='Third Field Introduction')\n",
        "                field_3_name_input = gr.Textbox(placeholder='Enter third field name...', label='Third Field Name')\n",
        "                field_3_content_input = gr.Textbox(placeholder='Enter third field content...', label='Third Field Content')\n",
        "            with gr.Column():\n",
        "                field_4_intro_input = gr.Textbox(placeholder='Enter fourth field introduction...', label='Fourth Field Introduction')\n",
        "                field_4_name_input = gr.Textbox(placeholder='Enter fourth field name...', label='Fourth Field Name')\n",
        "                field_4_content_input = gr.Textbox(placeholder='Enter fourth field content...', label='Fourth Field Content')\n",
        "        with gr.Row():\n",
        "            conclusion_input = gr.Textbox(placeholder='Enter conclusion...', label='Conclusion', scale=1)\n",
        "        with gr.Row():\n",
        "            output_area = gr.Textbox(label='Interaction Log', lines=10, scale=2, show_copy_button=True)\n",
        "\n",
        "        with gr.Row():\n",
        "            interaction_to_load = gr.Textbox(label='Interaction Timestamp', scale=2)\n",
        "            load_interaction_button = gr.Button(\"Load Interaction\", variant=\"primary\", scale=2)\n",
        "            refresh_button = gr.Button(\"Refresh List\", variant=\"secondary\", scale=2)\n",
        "        with gr.Row():\n",
        "            interaction_list = gr.Textbox(label='Interaction List', lines=10, scale=2)\n",
        "\n",
        "    # Event Listeners\n",
        "    refresh_button.click(\n",
        "        fn=lambda: read_interactions(),\n",
        "        inputs=[],\n",
        "        outputs=[interaction_list]\n",
        "    )\n",
        "    load_interaction_button.click(\n",
        "            fn=load_interaction_details,\n",
        "            inputs=[interaction_to_load],\n",
        "            outputs=[introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input,\n",
        "                     field_2_intro_input, field_2_name_input, field_2_content_input,\n",
        "                     field_3_intro_input, field_3_name_input, field_3_content_input,\n",
        "                     field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input, output_area]\n",
        "    )\n",
        "\n",
        "    log_interaction_button.click(\n",
        "        fn=log_and_display_interactions,\n",
        "        inputs=[autobackup_checkbox, template_name_input, model_input, temperature_input, max_tokens_input, introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input, field_2_intro_input, field_2_name_input, field_2_content_input, field_3_intro_input, field_3_name_input, field_3_content_input, field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input],\n",
        "        outputs=[output_area]\n",
        "    )\n",
        "    save_template_button.click(\n",
        "        fn=save_template_handler,\n",
        "        inputs=[autobackup_checkbox, template_name_input, introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input, field_2_intro_input, field_2_name_input, field_2_content_input, field_3_intro_input, field_3_name_input, field_3_content_input, field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input],\n",
        "        outputs=[status_area, template_selector]\n",
        "    )\n",
        "    clear_button.click(\n",
        "        fn=lambda: (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"),\n",
        "        inputs=[],\n",
        "        outputs=[template_name_input, introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input, field_2_intro_input, field_2_name_input, field_2_content_input, field_3_intro_input, field_3_name_input, field_3_content_input, field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input, output_area]\n",
        "    )\n",
        "    load_template_button.click(\n",
        "        fn=load_template_details,\n",
        "        inputs=[template_selector],\n",
        "        outputs=[introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input,\n",
        "                 field_2_intro_input, field_2_name_input, field_2_content_input,\n",
        "                 field_3_intro_input, field_3_name_input, field_3_content_input,\n",
        "                 field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input]\n",
        "    )\n",
        "    backup_button.click(\n",
        "        fn=lambda: backup_db_files('/content/', '/content/drive/MyDrive/InteractionTrackerBackups/'),\n",
        "        inputs=[],\n",
        "        outputs=[status_area]\n",
        "    )\n",
        "    restore_button.click(\n",
        "        fn=lambda: restore_db_files('/content/drive/MyDrive/InteractionTrackerBackups/', '/content/', 'interactions'),\n",
        "        inputs=[],\n",
        "        outputs=[status_area, template_selector]\n",
        "    )\n",
        "    restore_autobackup_button.click(\n",
        "        fn=lambda: restore_auto_backup('/content/drive/MyDrive/InteractionTrackerBackups/', '/content/', 'interactions'),\n",
        "        inputs=[],\n",
        "        outputs=[status_area, template_selector]\n",
        "    )\n",
        "    estimate_costs_button.click(\n",
        "        fn=combine_fields_and_estimate_costs,\n",
        "        inputs=[model_input, temperature_input, max_tokens_input, introduction_input, field_1_intro_input, field_1_name_input, field_1_content_input, field_2_intro_input, field_2_name_input, field_2_content_input, field_3_intro_input, field_3_name_input, field_3_content_input, field_4_intro_input, field_4_name_input, field_4_content_input, conclusion_input],\n",
        "        outputs=[status_area]\n",
        "    )\n",
        "    export_csv_button.click(\n",
        "        fn=lambda: backup_db_to_drive_as_csv('/content/', '/content/drive/MyDrive/InteractionTrackerBackups/'),\n",
        "        inputs=[],\n",
        "        outputs=[status_area]\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "I7XqY_c3RaQT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#launch the interface\n",
        "demo.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "wMwFep9rRgXU",
        "outputId": "b6899e5f-5d8d-477d-99f6-479fe4b6aa58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ccdf464f7046b60c82.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ccdf464f7046b60c82.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documentation for All the Buttons:\n",
        "\n",
        "\n",
        "1. **Refresh List Button:**\n",
        "   - **Dataflow:** This button triggers the `read_interactions()` function, which reads data from the `interactions` table in the SQLite database and displays a list of interaction timestamps and details in the `interaction_list` textbox.\n",
        "   - **Summary:** Refreshes and displays a list of all logged interactions.\n",
        "\n",
        "2. **Load Interaction Button:**\n",
        "   - **Dataflow:** Upon clicking, it takes the timestamp input from `interaction_to_load`, passes it to `load_interaction_details()`, and fills various text fields with details from the specified interaction record in the database.\n",
        "   - **Summary:** Loads the details of a specific interaction based on its timestamp for review or editing.\n",
        "\n",
        "3. **Log Interaction Button:**\n",
        "   - **Dataflow:** Gathers data from input fields related to interaction details, sends them to the `log_and_display_interactions()` function, which logs these details into the database and optionally triggers an automatic backup. It also makes a call to the OpenAI API for text generation.\n",
        "   - **Summary:** Logs details of an interaction and optionally backs up the database.\n",
        "\n",
        "4. **Save Template Button:**\n",
        "   - **Dataflow:** Collects data from template-related input fields, uses `save_template_handler()` to save these details as a new template in the database, and optionally performs an automatic backup.\n",
        "   - **Summary:** Saves the current form data as a reusable template and optionally backs up the data.\n",
        "\n",
        "5. **Clear Fields Button:**\n",
        "   - **Dataflow:** Resets all input fields in the form to their default empty state.\n",
        "   - **Summary:** Clears all the input fields for a fresh start on entering new data.\n",
        "\n",
        "6. **Load Template Button:**\n",
        "   - **Dataflow:** Takes the selected template name from the dropdown, retrieves its details from the database using `load_template_details()`, and populates the input fields with these details.\n",
        "   - **Summary:** Loads a selected template's details into the form for quick use or modification.\n",
        "\n",
        "7. **Backup Database Button:**\n",
        "   - **Dataflow:** Initiates the `backup_db_files()` function to copy the current state of the database files to a backup directory.\n",
        "   - **Summary:** Creates a backup of the current database state for recovery purposes.\n",
        "\n",
        "8. **Restore Database Button:**\n",
        "   - **Dataflow:** Uses `restore_db_files()` to replace the current database files with those from the most recent backup that has all necessary components (.db, .db-shm, .db-wal).\n",
        "   - **Summary:** Restores the database to its most recent complete backup state.\n",
        "\n",
        "9. **Restore Autobackup Button:**\n",
        "   - **Dataflow:** Invokes `restore_auto_backup()` to specifically restore files from an automatic backup process, ensuring the database is reverted to the last automatically saved state.\n",
        "   - **Summary:** Restores the database using the last set of automatically backed-up files.\n",
        "\n",
        "10. **Estimate Costs Button:**\n",
        "    - **Dataflow:** Aggregates the input from various fields to form a comprehensive description of an interaction, which is then used to calculate the cost of processing this text through the OpenAI API using `combine_fields_and_estimate_costs()`.\n",
        "    - **Summary:** Estimates the cost of generating responses using the specified AI model based on the input data.\n",
        "\n",
        "11. **Export CSV Button:**\n",
        "    - **Dataflow:** Converts the contents of the SQLite database into CSV files and saves them to a designated backup directory through the `backup_db_to_drive_as_csv()` function.\n",
        "    - **Summary:** Exports the database contents to CSV files for external use or analysis.\n"
      ],
      "metadata": {
        "id": "37ay5RPZRCKs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}