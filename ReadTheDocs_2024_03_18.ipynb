{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCLIGBK3QVUnJkhspvZCU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mindfulcoder49/NorthShoreAI/blob/main/ReadTheDocs_2024_03_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read The Docs Notebook for March 18 2024\n",
        "\n",
        "Hi! At the last meeting I mentioned that I might start creating Jupyter notebooks to learn llama-index because that's what worked for me the last time I tried to create a real project.\n",
        "\n",
        "I created this notebook, and I realized there are a few specific techniques I use when I'm doing this that are very different from just reading the documentation. In fact, it's more like generating the documentation you need to answer the questions you have.\n",
        "\n",
        "So I have left this notebook open ended so I can get it out today, on Wednesday, and leave the rest of the exploration up to you. I would love if people brough their own versions of this notebook and if they found the learning techniques here useful.\n",
        "\n",
        "Best Regards,\n",
        "Alex"
      ],
      "metadata": {
        "id": "fWKJMGsaX0CO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kugfM_s0_lGS",
        "outputId": "6527bcd9-b9ad-44ab-ab42-3925425f296a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install llama-index and import API Key\n",
        "\n",
        "One thing I do know is we need to install llama-index, so we did that first. The I guess it's time to start looking under the hood. First, we should make sure we have our API key set as an environment variable the way llama-index expects."
      ],
      "metadata": {
        "id": "DPYukfH9_34A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "#this method assumes you have the key set in your google colab secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "-55jOz_s_3Ep"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Documents: What do we get?\n",
        "\n",
        "The first thing llama_index suggests is to load your documents. That sounds simple enough, but I would like to know exactly what we are getting when we do that, so let's look:"
      ],
      "metadata": {
        "id": "Zu4HbKDLBHEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"sample_data\").load_data()"
      ],
      "metadata": {
        "id": "XAZUvUZrAoOp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print information about the documents variable\n",
        "length = len(documents)\n",
        "print(f\"Number of documents: {length}\")\n",
        "#type of each list element\n",
        "print(f\"Document type: {type(documents[0])}\")\n",
        "#properties in each document\n",
        "print(f\"Document properties: document.metadata {documents[0].metadata}\")\n",
        "#get a list of all document properties\n",
        "print(f\"Document properties: document.metadata.keys {documents[0].metadata.keys()}\")\n",
        "#document data\n",
        "print(f\"Document data: document.text {documents[0].text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esd3rwN-BUGG",
        "outputId": "3934b53f-f1c0-4447-f97d-2a82e64aa4ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 1\n",
            "Document type: <class 'llama_index.core.schema.Document'>\n",
            "Document properties: document.metadata {'file_path': '/content/sample_data/README.md', 'file_name': 'README.md', 'file_type': 'text/markdown', 'file_size': 930, 'creation_date': '2024-03-11', 'last_modified_date': '2000-01-01'}\n",
            "Document properties: document.metadata.keys dict_keys(['file_path', 'file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date'])\n",
            "Document data: document.text This directory includes a few sample datasets to get you started.\n",
            "\n",
            "*   `california_housing_data*.csv` is California housing data from the 1990 US\n",
            "    Census; more information is available at:\n",
            "    https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
            "\n",
            "*   `mnist_*.csv` is a small sample of the\n",
            "    MNIST database, which is\n",
            "    described at: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "*   `anscombe.json` contains a copy of\n",
            "    Anscombe's quartet; it\n",
            "    was originally described in\n",
            "\n",
            "    Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American\n",
            "    Statistician. 27 (1): 17-21. JSTOR 2682899.\n",
            "\n",
            "    and our copy was prepared by the\n",
            "    vega_datasets library.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Index using OpenAI\n",
        "\n",
        "llama-index is already solving problems for us. The SimpleDirectoryReader.loadData function gave us a list of Document objects, and each of the Document objects has a text property and a metadata property. The metadata property is a dict with the following keys:\n",
        "\n",
        "'file_path', 'file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date'\n",
        "\n",
        "Now let's make the index, and look at that. This will query OpenAI to generate the embeddings."
      ],
      "metadata": {
        "id": "BBeRWo7ECmBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "a2YWCWP1A_Qu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WaJ_QBJURl",
        "outputId": "9fba1947-96a9-4e96-fcef-43e0d30e630e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x797490378c70>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Index\n",
        "\n",
        "We can iterate over the list returned by the dir() function to see all the properties and methods of the VectorStoreIndex object used in LLamaIndex."
      ],
      "metadata": {
        "id": "d9u5GqptJlU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dir(index):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOtY6TElIETw",
        "outputId": "67781910-3d18-49c7-cb9e-8f7d2c24d2ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__abstractmethods__\n",
            "__annotations__\n",
            "__class__\n",
            "__class_getitem__\n",
            "__delattr__\n",
            "__dict__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__lt__\n",
            "__module__\n",
            "__ne__\n",
            "__new__\n",
            "__orig_bases__\n",
            "__parameters__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__sizeof__\n",
            "__slots__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__weakref__\n",
            "_abc_impl\n",
            "_add_nodes_to_index\n",
            "_aget_node_with_embedding\n",
            "_async_add_nodes_to_index\n",
            "_build_index_from_nodes\n",
            "_callback_manager\n",
            "_delete_node\n",
            "_docstore\n",
            "_embed_model\n",
            "_get_node_with_embedding\n",
            "_graph_store\n",
            "_index_struct\n",
            "_insert\n",
            "_insert_batch_size\n",
            "_is_protocol\n",
            "_object_map\n",
            "_service_context\n",
            "_show_progress\n",
            "_storage_context\n",
            "_store_nodes_override\n",
            "_transformations\n",
            "_use_async\n",
            "_vector_store\n",
            "as_chat_engine\n",
            "as_query_engine\n",
            "as_retriever\n",
            "build_index_from_nodes\n",
            "delete\n",
            "delete_nodes\n",
            "delete_ref_doc\n",
            "docstore\n",
            "from_documents\n",
            "from_vector_store\n",
            "index_id\n",
            "index_struct\n",
            "index_struct_cls\n",
            "insert\n",
            "insert_nodes\n",
            "ref_doc_info\n",
            "refresh\n",
            "refresh_ref_docs\n",
            "service_context\n",
            "set_index_id\n",
            "storage_context\n",
            "summary\n",
            "update\n",
            "update_ref_doc\n",
            "vector_store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at retrievers, query engines, and chat engines\n",
        "\n",
        "The things that catches my eye are the as_ehat_engine, as_query_engine, and as_retriever methods that will give me these three different things that are all used in LLamaIndex. This could be a good opportunity to learn the differences."
      ],
      "metadata": {
        "id": "XbhFERzQJM9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a retriever, query engine, and chat engine from my index and save them to different variables\n",
        "retriever = index.as_retriever()\n",
        "query_engine = index.as_query_engine()\n",
        "chat_engine = index.as_chat_engine()"
      ],
      "metadata": {
        "id": "RX1sme8fJMch"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we can take a look at what objects we get for each of these:"
      ],
      "metadata": {
        "id": "fXrV74dZPzTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print all objects\n",
        "print(retriever)\n",
        "print(query_engine)\n",
        "print(chat_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXfWbPj4PsM0",
        "outputId": "6971e028-0a81-4744-f806-d5b9ba694dc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever object at 0x7974486a8430>\n",
            "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7974486a9660>\n",
            "<llama_index.agent.openai.base.OpenAIAgent object at 0x7974470154b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first two are sort of intuitive, but the third is interesting. The chat_engine we get is an OpenAIAgent. That's the default, but I bet they tried to make it easy to use others.\n",
        "\n",
        "Let's take a look at the methods and properties for each one"
      ],
      "metadata": {
        "id": "9BVCMWBrQASz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dir(retriever):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYGexU9NI4cp",
        "outputId": "e151a65c-bfb1-47f7-a86d-21407a7e3ae7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__abstractmethods__\n",
            "__class__\n",
            "__delattr__\n",
            "__dict__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__lt__\n",
            "__module__\n",
            "__ne__\n",
            "__new__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__sizeof__\n",
            "__slots__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__weakref__\n",
            "_abc_impl\n",
            "_aget_nodes_with_embeddings\n",
            "_ahandle_recursive_retrieval\n",
            "_alpha\n",
            "_aretrieve\n",
            "_aretrieve_from_object\n",
            "_as_query_component\n",
            "_build_node_list_from_query_result\n",
            "_build_vector_store_query\n",
            "_check_callback_manager\n",
            "_doc_ids\n",
            "_docstore\n",
            "_embed_model\n",
            "_filters\n",
            "_get_nodes_with_embeddings\n",
            "_get_prompt_modules\n",
            "_get_prompts\n",
            "_handle_recursive_retrieval\n",
            "_index\n",
            "_kwargs\n",
            "_node_ids\n",
            "_retrieve\n",
            "_retrieve_from_object\n",
            "_similarity_top_k\n",
            "_sparse_top_k\n",
            "_update_prompts\n",
            "_validate_prompts\n",
            "_vector_store\n",
            "_vector_store_query_mode\n",
            "_verbose\n",
            "aretrieve\n",
            "as_query_component\n",
            "callback_manager\n",
            "get_prompts\n",
            "get_service_context\n",
            "object_map\n",
            "retrieve\n",
            "similarity_top_k\n",
            "update_prompts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for the query engine\n",
        "for item in dir(query_engine):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UOzbalNKDqN",
        "outputId": "c694d79f-59f7-4650-901d-e1fb7addc23a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__abstractmethods__\n",
            "__class__\n",
            "__delattr__\n",
            "__dict__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__lt__\n",
            "__module__\n",
            "__ne__\n",
            "__new__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__sizeof__\n",
            "__slots__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__weakref__\n",
            "_abc_impl\n",
            "_apply_node_postprocessors\n",
            "_aquery\n",
            "_as_query_component\n",
            "_get_prompt_modules\n",
            "_get_prompts\n",
            "_node_postprocessors\n",
            "_query\n",
            "_response_synthesizer\n",
            "_retriever\n",
            "_update_prompts\n",
            "_validate_prompts\n",
            "aquery\n",
            "aretrieve\n",
            "as_query_component\n",
            "asynthesize\n",
            "callback_manager\n",
            "from_args\n",
            "get_prompts\n",
            "query\n",
            "retrieve\n",
            "retriever\n",
            "synthesize\n",
            "update_prompts\n",
            "with_retriever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for the chat engine\n",
        "for item in dir(chat_engine):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x-g8XdqKQnb",
        "outputId": "3290ee45-a7e0-4a85-9758-f6cd9a48fcbf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__abstractmethods__\n",
            "__class__\n",
            "__delattr__\n",
            "__dict__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__lt__\n",
            "__module__\n",
            "__ne__\n",
            "__new__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__sizeof__\n",
            "__slots__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__weakref__\n",
            "_abc_impl\n",
            "_achat\n",
            "_aquery\n",
            "_arun_step\n",
            "_as_query_component\n",
            "_chat\n",
            "_get_prompt_modules\n",
            "_get_prompts\n",
            "_query\n",
            "_run_step\n",
            "_update_prompts\n",
            "_validate_prompts\n",
            "achat\n",
            "agent_worker\n",
            "aquery\n",
            "arun_step\n",
            "as_query_component\n",
            "astream_chat\n",
            "astream_step\n",
            "asynthesize\n",
            "callback_manager\n",
            "chat\n",
            "chat_history\n",
            "chat_repl\n",
            "create_task\n",
            "default_tool_choice\n",
            "delete_task\n",
            "delete_task_on_finish\n",
            "finalize_response\n",
            "from_llm\n",
            "from_tools\n",
            "get_completed_step\n",
            "get_completed_steps\n",
            "get_prompts\n",
            "get_task\n",
            "get_upcoming_steps\n",
            "init_task_state_kwargs\n",
            "list_tasks\n",
            "memory\n",
            "query\n",
            "reset\n",
            "retrieve\n",
            "run_step\n",
            "state\n",
            "stream_chat\n",
            "stream_step\n",
            "streaming_chat_repl\n",
            "synthesize\n",
            "undo_step\n",
            "update_prompts\n",
            "verbose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading through all these three lists helps me understand that the index is loaded into these objects to allow these methods we will want when we really start to think about what our app needs.\n",
        "\n",
        "One thing though, is that it would be nice to know which of these are properties and which are methods"
      ],
      "metadata": {
        "id": "DKcQGCh1Ky5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get each of the attributes and check if they are callable and output all of them labeled with either property or method\n",
        "my_objects = retriever, query_engine, chat_engine\n",
        "for my_object in my_objects:\n",
        "  #print the variable name and object\n",
        "  print(f\"Object: {my_object}\")\n",
        "  for attribute in dir(my_object):\n",
        "      # Use getattr to get the actual attribute value\n",
        "      attr_value = getattr(my_object, attribute)\n",
        "\n",
        "      # Check if the attribute is callable (method) or not (property)\n",
        "      if callable(attr_value):\n",
        "          print(f\"{attribute} is a method\")\n",
        "      else:\n",
        "          print(f\"{attribute} is a property\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jygE0tB_KXIp",
        "outputId": "48e07f06-f5d9-45d1-df14-138d6e29d7ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object: <llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever object at 0x7974486a8430>\n",
            "__abstractmethods__ is a property\n",
            "__class__ is a method\n",
            "__delattr__ is a method\n",
            "__dict__ is a property\n",
            "__dir__ is a method\n",
            "__doc__ is a property\n",
            "__eq__ is a method\n",
            "__format__ is a method\n",
            "__ge__ is a method\n",
            "__getattribute__ is a method\n",
            "__gt__ is a method\n",
            "__hash__ is a method\n",
            "__init__ is a method\n",
            "__init_subclass__ is a method\n",
            "__le__ is a method\n",
            "__lt__ is a method\n",
            "__module__ is a property\n",
            "__ne__ is a method\n",
            "__new__ is a method\n",
            "__reduce__ is a method\n",
            "__reduce_ex__ is a method\n",
            "__repr__ is a method\n",
            "__setattr__ is a method\n",
            "__sizeof__ is a method\n",
            "__slots__ is a property\n",
            "__str__ is a method\n",
            "__subclasshook__ is a method\n",
            "__weakref__ is a property\n",
            "_abc_impl is a property\n",
            "_aget_nodes_with_embeddings is a method\n",
            "_ahandle_recursive_retrieval is a method\n",
            "_alpha is a property\n",
            "_aretrieve is a method\n",
            "_aretrieve_from_object is a method\n",
            "_as_query_component is a method\n",
            "_build_node_list_from_query_result is a method\n",
            "_build_vector_store_query is a method\n",
            "_check_callback_manager is a method\n",
            "_doc_ids is a property\n",
            "_docstore is a property\n",
            "_embed_model is a method\n",
            "_filters is a property\n",
            "_get_nodes_with_embeddings is a method\n",
            "_get_prompt_modules is a method\n",
            "_get_prompts is a method\n",
            "_handle_recursive_retrieval is a method\n",
            "_index is a property\n",
            "_kwargs is a property\n",
            "_node_ids is a property\n",
            "_retrieve is a method\n",
            "_retrieve_from_object is a method\n",
            "_similarity_top_k is a property\n",
            "_sparse_top_k is a property\n",
            "_update_prompts is a method\n",
            "_validate_prompts is a method\n",
            "_vector_store is a property\n",
            "_vector_store_query_mode is a property\n",
            "_verbose is a property\n",
            "aretrieve is a method\n",
            "as_query_component is a method\n",
            "callback_manager is a property\n",
            "get_prompts is a method\n",
            "get_service_context is a method\n",
            "object_map is a property\n",
            "retrieve is a method\n",
            "similarity_top_k is a property\n",
            "update_prompts is a method\n",
            "Object: <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7974486a9660>\n",
            "__abstractmethods__ is a property\n",
            "__class__ is a method\n",
            "__delattr__ is a method\n",
            "__dict__ is a property\n",
            "__dir__ is a method\n",
            "__doc__ is a property\n",
            "__eq__ is a method\n",
            "__format__ is a method\n",
            "__ge__ is a method\n",
            "__getattribute__ is a method\n",
            "__gt__ is a method\n",
            "__hash__ is a method\n",
            "__init__ is a method\n",
            "__init_subclass__ is a method\n",
            "__le__ is a method\n",
            "__lt__ is a method\n",
            "__module__ is a property\n",
            "__ne__ is a method\n",
            "__new__ is a method\n",
            "__reduce__ is a method\n",
            "__reduce_ex__ is a method\n",
            "__repr__ is a method\n",
            "__setattr__ is a method\n",
            "__sizeof__ is a method\n",
            "__slots__ is a property\n",
            "__str__ is a method\n",
            "__subclasshook__ is a method\n",
            "__weakref__ is a property\n",
            "_abc_impl is a property\n",
            "_apply_node_postprocessors is a method\n",
            "_aquery is a method\n",
            "_as_query_component is a method\n",
            "_get_prompt_modules is a method\n",
            "_get_prompts is a method\n",
            "_node_postprocessors is a property\n",
            "_query is a method\n",
            "_response_synthesizer is a property\n",
            "_retriever is a property\n",
            "_update_prompts is a method\n",
            "_validate_prompts is a method\n",
            "aquery is a method\n",
            "aretrieve is a method\n",
            "as_query_component is a method\n",
            "asynthesize is a method\n",
            "callback_manager is a property\n",
            "from_args is a method\n",
            "get_prompts is a method\n",
            "query is a method\n",
            "retrieve is a method\n",
            "retriever is a property\n",
            "synthesize is a method\n",
            "update_prompts is a method\n",
            "with_retriever is a method\n",
            "Object: <llama_index.agent.openai.base.OpenAIAgent object at 0x7974470154b0>\n",
            "__abstractmethods__ is a property\n",
            "__class__ is a method\n",
            "__delattr__ is a method\n",
            "__dict__ is a property\n",
            "__dir__ is a method\n",
            "__doc__ is a property\n",
            "__eq__ is a method\n",
            "__format__ is a method\n",
            "__ge__ is a method\n",
            "__getattribute__ is a method\n",
            "__gt__ is a method\n",
            "__hash__ is a method\n",
            "__init__ is a method\n",
            "__init_subclass__ is a method\n",
            "__le__ is a method\n",
            "__lt__ is a method\n",
            "__module__ is a property\n",
            "__ne__ is a method\n",
            "__new__ is a method\n",
            "__reduce__ is a method\n",
            "__reduce_ex__ is a method\n",
            "__repr__ is a method\n",
            "__setattr__ is a method\n",
            "__sizeof__ is a method\n",
            "__slots__ is a property\n",
            "__str__ is a method\n",
            "__subclasshook__ is a method\n",
            "__weakref__ is a property\n",
            "_abc_impl is a property\n",
            "_achat is a method\n",
            "_aquery is a method\n",
            "_arun_step is a method\n",
            "_as_query_component is a method\n",
            "_chat is a method\n",
            "_get_prompt_modules is a method\n",
            "_get_prompts is a method\n",
            "_query is a method\n",
            "_run_step is a method\n",
            "_update_prompts is a method\n",
            "_validate_prompts is a method\n",
            "achat is a method\n",
            "agent_worker is a property\n",
            "aquery is a method\n",
            "arun_step is a method\n",
            "as_query_component is a method\n",
            "astream_chat is a method\n",
            "astream_step is a method\n",
            "asynthesize is a method\n",
            "callback_manager is a property\n",
            "chat is a method\n",
            "chat_history is a property\n",
            "chat_repl is a method\n",
            "create_task is a method\n",
            "default_tool_choice is a property\n",
            "delete_task is a method\n",
            "delete_task_on_finish is a property\n",
            "finalize_response is a method\n",
            "from_llm is a method\n",
            "from_tools is a method\n",
            "get_completed_step is a method\n",
            "get_completed_steps is a method\n",
            "get_prompts is a method\n",
            "get_task is a method\n",
            "get_upcoming_steps is a method\n",
            "init_task_state_kwargs is a property\n",
            "list_tasks is a method\n",
            "memory is a property\n",
            "query is a method\n",
            "reset is a method\n",
            "retrieve is a method\n",
            "run_step is a method\n",
            "state is a property\n",
            "stream_chat is a method\n",
            "stream_step is a method\n",
            "streaming_chat_repl is a method\n",
            "synthesize is a method\n",
            "undo_step is a method\n",
            "update_prompts is a method\n",
            "verbose is a property\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hunting down the source code for the interesting functions\n",
        "\n",
        "We might be able to get the same information from the documentation, but in this format we can ask for exactly the information we are interested in and ready to absorb. This is also the most authoritative source, and more trustworthy than the documentation.\n",
        "\n",
        "The source code is also a good read, and the base.py class is, in my opinion, and exciting and illuminating piece of code:\n",
        "\n",
        "https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/base.py\n",
        "\n",
        "I'll copy it below:"
      ],
      "metadata": {
        "id": "PkqnPPQMQlj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Base index classes.\"\"\"\n",
        "\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, Dict, Generic, List, Optional, Sequence, Type, TypeVar\n",
        "\n",
        "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
        "from llama_index.core.base.base_retriever import BaseRetriever\n",
        "from llama_index.core.callbacks.base import CallbackManager\n",
        "from llama_index.core.chat_engine.types import BaseChatEngine, ChatMode\n",
        "from llama_index.core.data_structs.data_structs import IndexStruct\n",
        "from llama_index.core.ingestion import run_transformations\n",
        "from llama_index.core.llms.utils import LLMType, resolve_llm\n",
        "from llama_index.core.schema import BaseNode, Document, IndexNode, TransformComponent\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.settings import (\n",
        "    Settings,\n",
        "    callback_manager_from_settings_or_context,\n",
        "    llm_from_settings_or_context,\n",
        "    transformations_from_settings_or_context,\n",
        ")\n",
        "from llama_index.core.storage.docstore.types import BaseDocumentStore, RefDocInfo\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "\n",
        "IS = TypeVar(\"IS\", bound=IndexStruct)\n",
        "IndexType = TypeVar(\"IndexType\", bound=\"BaseIndex\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class BaseIndex(Generic[IS], ABC):\n",
        "    \"\"\"Base LlamaIndex.\n",
        "\n",
        "    Args:\n",
        "        nodes (List[Node]): List of nodes to index\n",
        "        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
        "        service_context (ServiceContext): Service context container (contains\n",
        "            components like LLM, Embeddings, etc.).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    index_struct_cls: Type[IS]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nodes: Optional[Sequence[BaseNode]] = None,\n",
        "        objects: Optional[Sequence[IndexNode]] = None,\n",
        "        index_struct: Optional[IS] = None,\n",
        "        storage_context: Optional[StorageContext] = None,\n",
        "        callback_manager: Optional[CallbackManager] = None,\n",
        "        transformations: Optional[List[TransformComponent]] = None,\n",
        "        show_progress: bool = False,\n",
        "        # deprecated\n",
        "        service_context: Optional[ServiceContext] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize with parameters.\"\"\"\n",
        "        if index_struct is None and nodes is None and objects is None:\n",
        "            raise ValueError(\"One of nodes, objects, or index_struct must be provided.\")\n",
        "        if index_struct is not None and nodes is not None:\n",
        "            raise ValueError(\"Only one of nodes or index_struct can be provided.\")\n",
        "        # This is to explicitly make sure that the old UX is not used\n",
        "        if nodes is not None and len(nodes) >= 1 and not isinstance(nodes[0], BaseNode):\n",
        "            if isinstance(nodes[0], Document):\n",
        "                raise ValueError(\n",
        "                    \"The constructor now takes in a list of Node objects. \"\n",
        "                    \"Since you are passing in a list of Document objects, \"\n",
        "                    \"please use `from_documents` instead.\"\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(\"nodes must be a list of Node objects.\")\n",
        "\n",
        "        self._storage_context = storage_context or StorageContext.from_defaults()\n",
        "        # deprecated\n",
        "        self._service_context = service_context\n",
        "\n",
        "        self._docstore = self._storage_context.docstore\n",
        "        self._show_progress = show_progress\n",
        "        self._vector_store = self._storage_context.vector_store\n",
        "        self._graph_store = self._storage_context.graph_store\n",
        "        self._callback_manager = (\n",
        "            callback_manager\n",
        "            or callback_manager_from_settings_or_context(Settings, service_context)\n",
        "        )\n",
        "\n",
        "        objects = objects or []\n",
        "        self._object_map = {obj.index_id: obj.obj for obj in objects}\n",
        "        for obj in objects:\n",
        "            obj.obj = None  # clear the object to avoid serialization issues\n",
        "\n",
        "        with self._callback_manager.as_trace(\"index_construction\"):\n",
        "            if index_struct is None:\n",
        "                nodes = nodes or []\n",
        "                index_struct = self.build_index_from_nodes(\n",
        "                    nodes + objects  # type: ignore\n",
        "                )\n",
        "            self._index_struct = index_struct\n",
        "            self._storage_context.index_store.add_index_struct(self._index_struct)\n",
        "\n",
        "        self._transformations = (\n",
        "            transformations\n",
        "            or transformations_from_settings_or_context(Settings, service_context)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_documents(\n",
        "        cls: Type[IndexType],\n",
        "        documents: Sequence[Document],\n",
        "        storage_context: Optional[StorageContext] = None,\n",
        "        show_progress: bool = False,\n",
        "        callback_manager: Optional[CallbackManager] = None,\n",
        "        transformations: Optional[List[TransformComponent]] = None,\n",
        "        # deprecated\n",
        "        service_context: Optional[ServiceContext] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> IndexType:\n",
        "        \"\"\"Create index from documents.\n",
        "\n",
        "        Args:\n",
        "            documents (Optional[Sequence[BaseDocument]]): List of documents to\n",
        "                build the index from.\n",
        "\n",
        "        \"\"\"\n",
        "        storage_context = storage_context or StorageContext.from_defaults()\n",
        "        docstore = storage_context.docstore\n",
        "        callback_manager = (\n",
        "            callback_manager\n",
        "            or callback_manager_from_settings_or_context(Settings, service_context)\n",
        "        )\n",
        "        transformations = transformations or transformations_from_settings_or_context(\n",
        "            Settings, service_context\n",
        "        )\n",
        "\n",
        "        with callback_manager.as_trace(\"index_construction\"):\n",
        "            for doc in documents:\n",
        "                docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n",
        "\n",
        "            nodes = run_transformations(\n",
        "                documents,  # type: ignore\n",
        "                transformations,\n",
        "                show_progress=show_progress,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "            return cls(\n",
        "                nodes=nodes,\n",
        "                storage_context=storage_context,\n",
        "                callback_manager=callback_manager,\n",
        "                show_progress=show_progress,\n",
        "                transformations=transformations,\n",
        "                service_context=service_context,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def index_struct(self) -> IS:\n",
        "        \"\"\"Get the index struct.\"\"\"\n",
        "        return self._index_struct\n",
        "\n",
        "    @property\n",
        "    def index_id(self) -> str:\n",
        "        \"\"\"Get the index struct.\"\"\"\n",
        "        return self._index_struct.index_id\n",
        "\n",
        "    def set_index_id(self, index_id: str) -> None:\n",
        "        \"\"\"Set the index id.\n",
        "\n",
        "        NOTE: if you decide to set the index_id on the index_struct manually,\n",
        "        you will need to explicitly call `add_index_struct` on the `index_store`\n",
        "        to update the index store.\n",
        "\n",
        "        Args:\n",
        "            index_id (str): Index id to set.\n",
        "\n",
        "        \"\"\"\n",
        "        # delete the old index struct\n",
        "        old_id = self._index_struct.index_id\n",
        "        self._storage_context.index_store.delete_index_struct(old_id)\n",
        "        # add the new index struct\n",
        "        self._index_struct.index_id = index_id\n",
        "        self._storage_context.index_store.add_index_struct(self._index_struct)\n",
        "\n",
        "    @property\n",
        "    def docstore(self) -> BaseDocumentStore:\n",
        "        \"\"\"Get the docstore corresponding to the index.\"\"\"\n",
        "        return self._docstore\n",
        "\n",
        "    @property\n",
        "    def service_context(self) -> Optional[ServiceContext]:\n",
        "        return self._service_context\n",
        "\n",
        "    @property\n",
        "    def storage_context(self) -> StorageContext:\n",
        "        return self._storage_context\n",
        "\n",
        "    @property\n",
        "    def summary(self) -> str:\n",
        "        return str(self._index_struct.summary)\n",
        "\n",
        "    @summary.setter\n",
        "    def summary(self, new_summary: str) -> None:\n",
        "        self._index_struct.summary = new_summary\n",
        "        self._storage_context.index_store.add_index_struct(self._index_struct)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> IS:\n",
        "        \"\"\"Build the index from nodes.\"\"\"\n",
        "\n",
        "    def build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> IS:\n",
        "        \"\"\"Build the index from nodes.\"\"\"\n",
        "        self._docstore.add_documents(nodes, allow_update=True)\n",
        "        return self._build_index_from_nodes(nodes)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n",
        "        \"\"\"Index-specific logic for inserting nodes to the index struct.\"\"\"\n",
        "\n",
        "    def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n",
        "        \"\"\"Insert nodes.\"\"\"\n",
        "        for node in nodes:\n",
        "            if isinstance(node, IndexNode):\n",
        "                try:\n",
        "                    node.dict()\n",
        "                except ValueError:\n",
        "                    self._object_map[node.index_id] = node.obj\n",
        "                    node.obj = None\n",
        "\n",
        "        with self._callback_manager.as_trace(\"insert_nodes\"):\n",
        "            self.docstore.add_documents(nodes, allow_update=True)\n",
        "            self._insert(nodes, **insert_kwargs)\n",
        "            self._storage_context.index_store.add_index_struct(self._index_struct)\n",
        "\n",
        "    def insert(self, document: Document, **insert_kwargs: Any) -> None:\n",
        "        \"\"\"Insert a document.\"\"\"\n",
        "        with self._callback_manager.as_trace(\"insert\"):\n",
        "            nodes = run_transformations(\n",
        "                [document],\n",
        "                self._transformations,\n",
        "                show_progress=self._show_progress,\n",
        "            )\n",
        "\n",
        "            self.insert_nodes(nodes, **insert_kwargs)\n",
        "            self.docstore.set_document_hash(document.get_doc_id(), document.hash)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n",
        "        \"\"\"Delete a node.\"\"\"\n",
        "\n",
        "    def delete_nodes(\n",
        "        self,\n",
        "        node_ids: List[str],\n",
        "        delete_from_docstore: bool = False,\n",
        "        **delete_kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Delete a list of nodes from the index.\n",
        "\n",
        "        Args:\n",
        "            doc_ids (List[str]): A list of doc_ids from the nodes to delete\n",
        "\n",
        "        \"\"\"\n",
        "        for node_id in node_ids:\n",
        "            self._delete_node(node_id, **delete_kwargs)\n",
        "            if delete_from_docstore:\n",
        "                self.docstore.delete_document(node_id, raise_error=False)\n",
        "\n",
        "        self._storage_context.index_store.add_index_struct(self._index_struct)\n",
        "\n",
        "    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n",
        "        \"\"\"Delete a document from the index.\n",
        "        All nodes in the index related to the index will be deleted.\n",
        "\n",
        "        Args:\n",
        "            doc_id (str): A doc_id of the ingested document\n",
        "\n",
        "        \"\"\"\n",
        "        logger.warning(\n",
        "            \"delete() is now deprecated, please refer to delete_ref_doc() to delete \"\n",
        "            \"ingested documents+nodes or delete_nodes to delete a list of nodes.\"\n",
        "        )\n",
        "        self.delete_ref_doc(doc_id)\n",
        "\n",
        "    def delete_ref_doc(\n",
        "        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n",
        "        ref_doc_info = self.docstore.get_ref_doc_info(ref_doc_id)\n",
        "        if ref_doc_info is None:\n",
        "            logger.warning(f\"ref_doc_id {ref_doc_id} not found, nothing deleted.\")\n",
        "            return\n",
        "\n",
        "        self.delete_nodes(\n",
        "            ref_doc_info.node_ids,\n",
        "            delete_from_docstore=False,\n",
        "            **delete_kwargs,\n",
        "        )\n",
        "\n",
        "        if delete_from_docstore:\n",
        "            self.docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n",
        "\n",
        "    def update(self, document: Document, **update_kwargs: Any) -> None:\n",
        "        \"\"\"Update a document and it's corresponding nodes.\n",
        "\n",
        "        This is equivalent to deleting the document and then inserting it again.\n",
        "\n",
        "        Args:\n",
        "            document (Union[BaseDocument, BaseIndex]): document to update\n",
        "            insert_kwargs (Dict): kwargs to pass to insert\n",
        "            delete_kwargs (Dict): kwargs to pass to delete\n",
        "\n",
        "        \"\"\"\n",
        "        logger.warning(\n",
        "            \"update() is now deprecated, please refer to update_ref_doc() to update \"\n",
        "            \"ingested documents+nodes.\"\n",
        "        )\n",
        "        self.update_ref_doc(document, **update_kwargs)\n",
        "\n",
        "    def update_ref_doc(self, document: Document, **update_kwargs: Any) -> None:\n",
        "        \"\"\"Update a document and it's corresponding nodes.\n",
        "\n",
        "        This is equivalent to deleting the document and then inserting it again.\n",
        "\n",
        "        Args:\n",
        "            document (Union[BaseDocument, BaseIndex]): document to update\n",
        "            insert_kwargs (Dict): kwargs to pass to insert\n",
        "            delete_kwargs (Dict): kwargs to pass to delete\n",
        "\n",
        "        \"\"\"\n",
        "        with self._callback_manager.as_trace(\"update\"):\n",
        "            self.delete_ref_doc(\n",
        "                document.get_doc_id(),\n",
        "                delete_from_docstore=True,\n",
        "                **update_kwargs.pop(\"delete_kwargs\", {}),\n",
        "            )\n",
        "            self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n",
        "\n",
        "    def refresh(\n",
        "        self, documents: Sequence[Document], **update_kwargs: Any\n",
        "    ) -> List[bool]:\n",
        "        \"\"\"Refresh an index with documents that have changed.\n",
        "\n",
        "        This allows users to save LLM and Embedding model calls, while only\n",
        "        updating documents that have any changes in text or metadata. It\n",
        "        will also insert any documents that previously were not stored.\n",
        "        \"\"\"\n",
        "        logger.warning(\n",
        "            \"refresh() is now deprecated, please refer to refresh_ref_docs() to \"\n",
        "            \"refresh ingested documents+nodes with an updated list of documents.\"\n",
        "        )\n",
        "        return self.refresh_ref_docs(documents, **update_kwargs)\n",
        "\n",
        "    def refresh_ref_docs(\n",
        "        self, documents: Sequence[Document], **update_kwargs: Any\n",
        "    ) -> List[bool]:\n",
        "        \"\"\"Refresh an index with documents that have changed.\n",
        "\n",
        "        This allows users to save LLM and Embedding model calls, while only\n",
        "        updating documents that have any changes in text or metadata. It\n",
        "        will also insert any documents that previously were not stored.\n",
        "        \"\"\"\n",
        "        with self._callback_manager.as_trace(\"refresh\"):\n",
        "            refreshed_documents = [False] * len(documents)\n",
        "            for i, document in enumerate(documents):\n",
        "                existing_doc_hash = self._docstore.get_document_hash(\n",
        "                    document.get_doc_id()\n",
        "                )\n",
        "                if existing_doc_hash is None:\n",
        "                    self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n",
        "                    refreshed_documents[i] = True\n",
        "                elif existing_doc_hash != document.hash:\n",
        "                    self.update_ref_doc(\n",
        "                        document, **update_kwargs.pop(\"update_kwargs\", {})\n",
        "                    )\n",
        "                    refreshed_documents[i] = True\n",
        "\n",
        "            return refreshed_documents\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n",
        "        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n",
        "        ...\n",
        "\n",
        "    def as_query_engine(\n",
        "        self, llm: Optional[LLMType] = None, **kwargs: Any\n",
        "    ) -> BaseQueryEngine:\n",
        "        # NOTE: lazy import\n",
        "        from llama_index.core.query_engine.retriever_query_engine import (\n",
        "            RetrieverQueryEngine,\n",
        "        )\n",
        "\n",
        "        retriever = self.as_retriever(**kwargs)\n",
        "        llm = (\n",
        "            resolve_llm(llm, callback_manager=self._callback_manager)\n",
        "            if llm\n",
        "            else llm_from_settings_or_context(Settings, self.service_context)\n",
        "        )\n",
        "\n",
        "        return RetrieverQueryEngine.from_args(\n",
        "            retriever,\n",
        "            llm=llm,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def as_chat_engine(\n",
        "        self,\n",
        "        chat_mode: ChatMode = ChatMode.BEST,\n",
        "        llm: Optional[LLMType] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> BaseChatEngine:\n",
        "        service_context = kwargs.get(\"service_context\", self.service_context)\n",
        "\n",
        "        if service_context is not None:\n",
        "            llm = (\n",
        "                resolve_llm(llm, callback_manager=self._callback_manager)\n",
        "                if llm\n",
        "                else service_context.llm\n",
        "            )\n",
        "        else:\n",
        "            llm = (\n",
        "                resolve_llm(llm, callback_manager=self._callback_manager)\n",
        "                if llm\n",
        "                else Settings.llm\n",
        "            )\n",
        "\n",
        "        query_engine = self.as_query_engine(llm=llm, **kwargs)\n",
        "\n",
        "        # resolve chat mode\n",
        "        if chat_mode in [ChatMode.REACT, ChatMode.OPENAI, ChatMode.BEST]:\n",
        "            # use an agent with query engine tool in these chat modes\n",
        "            # NOTE: lazy import\n",
        "            from llama_index.core.agent import AgentRunner\n",
        "            from llama_index.core.tools.query_engine import QueryEngineTool\n",
        "\n",
        "            # convert query engine to tool\n",
        "            query_engine_tool = QueryEngineTool.from_defaults(query_engine=query_engine)\n",
        "\n",
        "            return AgentRunner.from_llm(\n",
        "                tools=[query_engine_tool],\n",
        "                llm=llm,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        if chat_mode == ChatMode.CONDENSE_QUESTION:\n",
        "            # NOTE: lazy import\n",
        "            from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
        "\n",
        "            return CondenseQuestionChatEngine.from_defaults(\n",
        "                query_engine=query_engine,\n",
        "                llm=llm,\n",
        "                **kwargs,\n",
        "            )\n",
        "        elif chat_mode == ChatMode.CONTEXT:\n",
        "            from llama_index.core.chat_engine import ContextChatEngine\n",
        "\n",
        "            return ContextChatEngine.from_defaults(\n",
        "                retriever=self.as_retriever(**kwargs),\n",
        "                llm=llm,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        elif chat_mode == ChatMode.CONDENSE_PLUS_CONTEXT:\n",
        "            from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
        "\n",
        "            return CondensePlusContextChatEngine.from_defaults(\n",
        "                retriever=self.as_retriever(**kwargs),\n",
        "                llm=llm,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        elif chat_mode == ChatMode.SIMPLE:\n",
        "            from llama_index.core.chat_engine import SimpleChatEngine\n",
        "\n",
        "            return SimpleChatEngine.from_defaults(\n",
        "                llm=llm,\n",
        "                **kwargs,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown chat mode: {chat_mode}\")\n",
        "\n",
        "\n",
        "# legacy\n",
        "BaseGPTIndex = BaseIndex"
      ],
      "metadata": {
        "id": "gyZ0eSEgMBy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the Documentation you need\n",
        "\n",
        "Here is some documentation generated with GPT-4:\n",
        "\n",
        "The code you've provided defines the `BaseIndex` class, which is a generic, abstract base class for constructing indexes in the LlamaIndex system. Here's a brief explanation of each method:\n",
        "\n",
        "- **`__init__`**: Initializes the `BaseIndex` instance. It checks for the validity of input parameters (nodes, objects, or index_struct must be provided) and sets up storage, service contexts, and callback managers. It builds or sets the index structure (`_index_struct`) and processes transformation components.\n",
        "\n",
        "- **`from_documents`**: A class method that creates an index from a sequence of documents. It processes the documents through transformations and initializes an index with the resulting nodes.\n",
        "\n",
        "- **`index_struct`**: A property that returns the index structure associated with this index.\n",
        "\n",
        "- **`index_id`**: A property that returns the unique identifier of the index structure.\n",
        "\n",
        "- **`set_index_id`**: Allows setting a new unique identifier for the index, updating the storage context accordingly.\n",
        "\n",
        "- **`docstore`**: Provides access to the document store associated with this index.\n",
        "\n",
        "- **`service_context`**: Returns the service context associated with the index.\n",
        "\n",
        "- **`storage_context`**: Returns the storage context associated with the index.\n",
        "\n",
        "- **`summary`**: A property that provides a summary of the index structure. It's also writable, allowing you to update the summary.\n",
        "\n",
        "- **`_build_index_from_nodes`**: An abstract method that must be implemented by subclasses to define how the index is built from nodes.\n",
        "\n",
        "- **`build_index_from_nodes`**: Takes a sequence of nodes and builds the index from them. It updates the document store and constructs the index structure using `_build_index_from_nodes`.\n",
        "\n",
        "- **`_insert`**: An abstract method for inserting nodes into the index, with subclass-specific logic.\n",
        "\n",
        "- **`insert_nodes`**: Inserts a sequence of nodes into the index, updating the document store and index structure as needed.\n",
        "\n",
        "- **`insert`**: Inserts a single document into the index, transforming it into nodes before inserting.\n",
        "\n",
        "- **`_delete_node`**: An abstract method for deleting a single node from the index.\n",
        "\n",
        "- **`delete_nodes`**: Deletes a list of nodes from the index, optionally removing them from the document store as well.\n",
        "\n",
        "- **`delete`**: Deprecated method for deleting a document from the index by its document ID.\n",
        "\n",
        "- **`delete_ref_doc`**: Deletes a referenced document and its associated nodes from the index and, optionally, the document store.\n",
        "\n",
        "- **`update`**: Deprecated method for updating a document and its nodes in the index.\n",
        "\n",
        "- **`update_ref_doc`**: Updates a referenced document and its nodes in the index, effectively reinserting it.\n",
        "\n",
        "- **`refresh`**: Deprecated method for refreshing an index with updated documents.\n",
        "\n",
        "- **`refresh_ref_docs`**: Updates the index with a list of documents that have changed, optimizing the use of LLM and embedding model calls.\n",
        "\n",
        "- **`ref_doc_info`**: An abstract property that should provide mapping information for referenced documents and their nodes.\n",
        "\n",
        "- **`as_retriever`**: An abstract method that should return a retriever instance based on the index.\n",
        "\n",
        "- **`as_query_engine`**: Creates a query engine for the index, allowing for complex queries using a retriever and possibly a large language model (LLM).\n",
        "\n",
        "- **`as_chat_engine`**: Sets up a chat engine with various modes, utilizing the index for retrieving information and a large language model for generating responses.\n",
        "\n",
        "These methods provide a comprehensive interface for building, managing, and utilizing indexes in the LlamaIndex system, facilitating complex data retrieval and interaction scenarios."
      ],
      "metadata": {
        "id": "BxIeflO2WtNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also generated more specific documentation on the three methods we used above:\n",
        "\n",
        "The provided code snippet outlines the architecture of a sophisticated indexing system designed to work with various types of data and leverage large language models (LLMs) for processing. Here's an expansion on the key components you've asked about:\n",
        "\n",
        "### as_retriever Method\n",
        "- **Purpose**: This abstract method is intended to return a `BaseRetriever` instance specific to the index. A retriever is a component that allows for efficient fetching or searching of data from the index based on queries.\n",
        "- **How It Works**: When implemented, this method configures and initializes a retriever that is capable of interacting with the index's underlying structure (document store, vector store, etc.) to retrieve relevant information or documents based on the input queries. This is crucial for building search and query functionalities on top of the indexed data.\n",
        "\n",
        "### as_query_engine Method\n",
        "- **Purpose**: Creates a `BaseQueryEngine` instance that facilitates complex queries over the index, potentially using a retriever and a large language model. This engine can handle more sophisticated queries than simple retrievals, including those that require understanding and processing natural language queries.\n",
        "- **How It Works**: This method leverages the retriever (obtained via `as_retriever`) and optionally integrates a large language model to process and interpret complex queries. The query engine can perform tasks such as parsing natural language questions, determining the intent of queries, and using the retriever to fetch relevant data. The integration of LLMs allows for a more nuanced understanding and processing of queries, thereby enhancing the query capabilities beyond what's possible with traditional search algorithms.\n",
        "\n",
        "### as_chat_engine Method\n",
        "- **Purpose**: Sets up a `BaseChatEngine` with various modes, utilizing the index for retrieving information and a large language model for generating responses. This is geared towards building conversational interfaces that can interact intelligently with users, providing answers or information based on the indexed data.\n",
        "- **How It Works**: The method configures a chat engine that can operate in different modes (e.g., `ChatMode.BEST`, `ChatMode.REACT`, etc.), depending on the requirements. It uses the query engine (set up via `as_query_engine`) to fetch relevant information from the index in response to user queries. The LLM plays a critical role here by generating conversational responses that are informed by the data retrieved from the index. This enables the development of interactive and intelligent chat-based interfaces or bots that can provide users with information, answer questions, or assist with tasks based on the indexed data.\n",
        "\n",
        "Each of these components plays a critical role in building a comprehensive and interactive data indexing and retrieval system. They enable the construction of sophisticated applications capable of understanding and responding to complex queries, supporting conversational interfaces, and leveraging the power of large language models to enhance user interaction and information retrieval."
      ],
      "metadata": {
        "id": "ndldhSBYW9-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions for Monday March 18:\n",
        "\n",
        "1. What exactly are documents and nodes and when are they used?\n",
        "2. What are the service context and storage context and when are they used?\n",
        "3. What other chat agents are available in as_chat_engine?"
      ],
      "metadata": {
        "id": "Jg52GB-MXPom"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLXYL6isWwcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}