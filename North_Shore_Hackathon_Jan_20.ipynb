{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF/79jZk8c808e/g7lFHFN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mindfulcoder49/NorthShoreAI/blob/main/North_Shore_Hackathon_Jan_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Full Stack App in Google Colab with OpenAI, Gradio, and Dataset"
      ],
      "metadata": {
        "id": "LwLpcsy3rTIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was initially created for the first meeting of the North Shore AI, Machine Learning, and Coding Meetup. It is designed to be interesting for anyone from beginners to experts. There are some very brief explanations of coding syntax for anyone with no experience with python or jupyter notebook.\n",
        "\n",
        "If you are not familiar with Jupyter notebooks, this is a notebook that includes text and code blocks. The code blocks are meant to be run in order. There is a little \"Play\" button to the left of each code block that allows you to \"run\" that block. As you read through the notebooks, run the code blocks in order."
      ],
      "metadata": {
        "id": "iv4EyzYjnH_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "The reason we can build a whole app in just a few lines of code in python is because other people have spent a long time creating packages of python code for us to use. We need to install those packages in this Google Colab environment. We do that in python using pip. We use a ! before pip to tell Google Colab this is a shell command, not python code."
      ],
      "metadata": {
        "id": "akYcvUq8Pyl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sqlalchemy>=2.0 dataset gradio openai"
      ],
      "metadata": {
        "id": "xSbGEilEwJ-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding your OpenAI API Key\n",
        "\n",
        "Querying the OpenAI API is easy by design, and the difficult part is getting an API key. An API key is basically a password that your send with your API request to OpenAI that is connected to your OpenAI account. It is how OpenAI makes sure only authorized users are using its API, it tracks your usage, and OpenAI uses that information to charge you accordingly for API usage. I have put $10 into my OpenAI account and have managed to spend about 25 cents of that developing this notebook. This key will be active for the duration of the workshop:\n",
        "\n",
        "```\n",
        "key goes here\n",
        "```\n",
        "\n",
        "You will need to add this key to the notebook:\n",
        "\n",
        "1) Click on the Key icon on the left\n",
        "2) Click \"+ Add new secret\"\n",
        "3) Enter the name OPENAI_API_KEY\n",
        "4) Paste in the API key above for the Value\n",
        "5) Click the round toggle on the left to activate Notebook Access\n",
        "\n",
        "later we will use this code to get the key and pass it to OpenAI:\n",
        "\n",
        "```\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "```"
      ],
      "metadata": {
        "id": "V9tkXm9_ir9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying an OpenAI API\n",
        "\n",
        "Querying the OpenAI API is very simple. First let's assign our message to a variable. In Python we put text in between single or double quotes, and to save that text to a variable, we write it with an equals sign like this:"
      ],
      "metadata": {
        "id": "BxULTvQUPfdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_message = \"Hello ChatGPT, what are your plans for humanity?\"\n",
        "\n",
        "print(my_message)"
      ],
      "metadata": {
        "id": "ix_nzrIeQzK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to tell the api which OpenAI model you want to query. You can find the different models here: https://platform.openai.com/docs/models\n",
        "\n",
        "GPT-3 models are significantly less expensive than GPT-4 models, so we are going to stick to using GPT-3 models here."
      ],
      "metadata": {
        "id": "pCNVVdsehQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = \"gpt-3.5-turbo-1106\""
      ],
      "metadata": {
        "id": "naXwEzHlhjiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need to tell the GPT API how many token we want back in our response. This is because the API charges you per token, and this lets you control that cost directly. Let's set it to 60 now, but you can change that if you want.\n",
        "\n",
        "When picking variable names, you're save with letters and underscores. When saving a number to a variable, you just write the number, like this:"
      ],
      "metadata": {
        "id": "syT1Jv4BgTVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "how_many_tokens_you_want = 60"
      ],
      "metadata": {
        "id": "2arWU9c-gSzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the code for calling the API is fairly standard. Take a moment to read through the code and notice this is just three function calls. We call the OpenAI function with our API key to create a client object, then we call the client.chat.completions.create function with our message, model name, and token limit, to query the API and get a response and then we call the print function to show the response content."
      ],
      "metadata": {
        "id": "10mwCIqig1vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "response = client.chat.completions.create(\n",
        "  model = my_model,\n",
        "  messages = [{\"role\":\"user\",\"content\":my_message}],\n",
        "  max_tokens = how_many_tokens_you_want\n",
        ")\n",
        "\n",
        "print( response.choices[0].message.content )\n"
      ],
      "metadata": {
        "id": "q2giTM33Pn4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Gradio, to give us a Front End\n",
        "\n",
        "We have some code now that let's us interact with GPT-3.5, but we need a nice interface if this is going to be a real webapp and not just a piece of python code.\n",
        "\n",
        "Gradio makes that easy. But we need to first make a python function.\n",
        "\n",
        "We have been using python function this whole time like when we call print(my_message). Now we put our call to OpenAI inside a function:"
      ],
      "metadata": {
        "id": "24lojSIDYofa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gpt(user_input):\n",
        "\n",
        "  client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",  # Replace with the correct GPT-4 model name\n",
        "    messages=[{\"role\":\"user\",\"content\":user_input}],\n",
        "    max_tokens=60\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "rTsTD1TKYtY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can use this function to chat with GPT in a more simple way:"
      ],
      "metadata": {
        "id": "RyxT4jhLZoK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_with_gpt(\"Hello GPT, how are you?\"))"
      ],
      "metadata": {
        "id": "yFlM_ZHRZucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_with_gpt(\"What is cassava?\"))"
      ],
      "metadata": {
        "id": "VHtxkjSKZ5pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have a function, creating a front end interface with Gradio is easy!"
      ],
      "metadata": {
        "id": "EcpGqxczaCfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "simplechat = gr.Interface(\n",
        "    fn=chat_with_gpt,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Your Message\"),\n",
        "    outputs=gr.Textbox(label=\"GPT Response\"),\n",
        "    title=\"Chat with GPT\",\n",
        "    description=\"This is a simple chat app using Gradio and OpenAI's GPT\"\n",
        ")"
      ],
      "metadata": {
        "id": "LtURYgSzaO98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.TabbedInterface(\n",
        "    [simplechat], [\"SimpleChat\"]\n",
        ").launch()"
      ],
      "metadata": {
        "id": "r8PgjI1UaYEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a nice interface and an easy way to chat with GPT. But you might have noticed that there is no conversation history, and our AI chatbot doesn't remember anything we say. Every message is interpreted as the beginning of a chat conversation, and that's not what we want."
      ],
      "metadata": {
        "id": "0ql7tZHMcRjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a chat conversation\n",
        "\n",
        "Building a chat conversation is simple, but requires two more python data types:\n",
        "an array, which is a list, and uses square brackets and commas:\n",
        "\n",
        "```\n",
        "[\"one\", \"two\", \"three\", \"four\"]\n",
        "```\n",
        "\n",
        "and a dictionary, which is a collection of key and value pairs, and uses curly brackets, colons, and commas:\n",
        "\n",
        "```\n",
        "{\"one\":\"apple\", \"two\":\"orange\", \"three\":\"banana\"}\n",
        "```\n",
        "\n",
        "When we send our messages to the OpenAI API, each message is a dictionary with the keys \"role\" and \"content\", and we will use the values \"user\" or \"assistant\" for the role, and the content will be the message:\n",
        "\n",
        "e.g.\n",
        "```\n",
        "{\"role\":\"user\", \"content\":\"What is your name?\"}\n",
        "```\n",
        "\n",
        "We will then put all the messages in our conversation in an array:\n",
        "\n",
        "e.g.\n",
        "```\n",
        "[{\"role\":\"user\", \"content\":\"What is your name?\"},\n",
        " {\"role\":\"assistant\", \"content\":\"My name is GPT-3\"},\n",
        " {\"role\":\"user\", \"content\":\"Nice to meet you\"}]\n",
        "```\n",
        "To create a conversation, we must send the entire conversation to OpenAI every single time we call the API to get the next response from GPT-3.5"
      ],
      "metadata": {
        "id": "VAZlXNekSM1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "first_user_message = {\"role\":\"user\", \"content\":\"Hello, what is your name?\"}\n",
        "current_messages = [first_user_message]\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "response = client.chat.completions.create(\n",
        "  model = \"gpt-3.5-turbo-1106\",  # Replace with the correct GPT-4 model name\n",
        "  messages = current_messages,\n",
        "  max_tokens = 60\n",
        ")\n",
        "\n",
        "first_AI_Response = response.choices[0].message\n",
        "\n",
        "print(first_AI_Response)\n"
      ],
      "metadata": {
        "id": "XijreA04WSN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in the code above is that we saved the AI response in the first_AI_Response variable so we can add it to our message array in the next call to the API:"
      ],
      "metadata": {
        "id": "CHf6c9XcXEU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_user_message = {\"role\":\"user\",\"content\":\"That's a funny name, where did you get it?\"}\n",
        "\n",
        "current_messages = [first_user_message, first_AI_Response, second_user_message]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",  # Replace with the correct GPT-4 model name\n",
        "  messages=current_messages,\n",
        "  max_tokens=60\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "b4Ftofq3V_7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a way to save the response from the previous message and add it to the next message inside of the function we are using in our Gradio Interface."
      ],
      "metadata": {
        "id": "wJv0wt48YLyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Dataset, to give us a database"
      ],
      "metadata": {
        "id": "Bdp_ryEiYdKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The traditional solution to this classic problem is to create a database. This completes our stack. When people talk about full stack development, they are talking about building the front end, the functions in the middle, and the database where information is stored. Our frontend is Gradio, and our functions in the middle are using the OpenAI API and now will also use Dataset to manage chat history.\n",
        "\n",
        "We will use four dataset functions.\n",
        "\n",
        "First we use dataset.connect to create a database file:\n",
        "\n",
        "```\n",
        "db = dataset.connect('sqlite:///chat_history.db)\n",
        "```\n",
        "\n",
        "Then we create a table:\n",
        "\n",
        "```\n",
        "table = db['history']\n",
        "```\n",
        "\n",
        "In our function we use table.all() to get all rows of the table. Each row will contain a message and the AI's response, and we will use a loop to restructure that data before sending it to OpenAI:\n",
        "\n",
        "```\n",
        "table.all()\n",
        "```\n",
        "\n",
        "And then when we receive each response from the AI, we save the latest message and response pair as a new table row using table.insert():\n",
        "\n",
        "```\n",
        "table.inset(dict(user_input=user_input, assistant_response=gpt_response))\n",
        "```\n",
        "\n",
        "If you have ever dealt with setting up a database, defining your tables, and building functions to get data, you can see how this is easier."
      ],
      "metadata": {
        "id": "t2t93UyadFbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a whole app in one go\n",
        "\n",
        "The code below integrates dataset and some more complex logic to create a chat interface that includes a chat history and gives the user an experience of chatting with an AI who can remember the whole conversation.\n",
        "\n",
        "If you'd like, take some time to read through the code line by line and make sure you know what each line is doing. If you're not sure about it, try asking the GPT in the interface you made!"
      ],
      "metadata": {
        "id": "l2GEIaEnPovg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import dataset\n",
        "from google.colab import userdata\n",
        "\n",
        "# Instantiate the OpenAI client\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))  # Replace with your API key\n",
        "\n",
        "# Connect to a SQLite database (it will be created if it doesn't exist)\n",
        "db = dataset.connect('sqlite:///chat_history.db')\n",
        "table = db['history']\n",
        "\n",
        "# Function to handle the chat and interact with the database\n",
        "def chat_with_gpt3_5(user_input):\n",
        "    # Retrieve and format the chat history from the database\n",
        "    chat_history = []\n",
        "    for row in table.all():\n",
        "        chat_history.append({'role': 'user', 'content': row['user_input']})\n",
        "        if row['assistant_response']:\n",
        "            chat_history.append({'role': 'assistant', 'content': row['assistant_response']})\n",
        "\n",
        "    # Add the current user input to the chat history\n",
        "    chat_history.append({'role': 'user', 'content': user_input})\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=chat_history,\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        gpt_response = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        gpt_response = f\"Error: {str(e)}\"\n",
        "\n",
        "    # Store new user input and assistant response in the database\n",
        "    table.insert(dict(user_input=user_input, assistant_response=gpt_response))\n",
        "\n",
        "    # Update and format the chat history for display\n",
        "    chat_history.append({'role': 'assistant', 'content': gpt_response})\n",
        "    formatted_chat_history = \"\\n\".join([f\"{msg['role'].title()}: {msg['content']}\" for msg in chat_history])\n",
        "\n",
        "    return formatted_chat_history\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_gpt3_5,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Your Message\"),\n",
        "    outputs=gr.Textbox(label=\"Chat History\"),\n",
        "    title=\"Chat with GPT-3.5\",\n",
        "    description=\"This is a simple chat app using Gradio and OpenAI's GPT-3.5, with chat history stored in SQLite via Dataset.\"\n",
        ")\n",
        "\n",
        "gr.TabbedInterface(\n",
        "    [iface], [\"HistoryChat\"]\n",
        ").launch()"
      ],
      "metadata": {
        "id": "y6FRGpWrwHNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have a second Gradio app running in the same notebook, courtesy of their amazing TabbedInterface function. Try making some changes to the API call like increasing the max_tokens to get longer answers or changing the temperature value to see what it does. Use the app to send code snippets and questions to GPT for guidance!"
      ],
      "metadata": {
        "id": "hRx13GcwmIZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building your own app\n",
        "\n",
        "Below is basically a copy of the HistoryChat app, with the gradio interface and function renamed to prevent naming conflicts. Try making some changes to explore how the app works, and maybe to build a new app of your own.\n",
        "\n",
        "Try using the HistoryChat above to get help from GPT, and feel free to increase the max_tokens parameter to get longer answers to your questions!\n",
        "\n",
        "One thing to notice: Since you're using the same database in this cloned app, you'll see the same chat history as in the one above! You can change the name of the database file to make a new chat history."
      ],
      "metadata": {
        "id": "-zDIw6HJcOtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import dataset\n",
        "from google.colab import userdata\n",
        "\n",
        "# Instantiate the OpenAI client\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))  # Replace with your API key\n",
        "\n",
        "# Connect to a SQLite database (it will be created if it doesn't exist)\n",
        "db = dataset.connect('sqlite:///chat_history.db')\n",
        "table = db['history']\n",
        "\n",
        "# Function to handle the chat and interact with the database\n",
        "def my_gradio_function(user_input):\n",
        "    # Retrieve and format the chat history from the database\n",
        "    chat_history = []\n",
        "    for row in table.all():\n",
        "        chat_history.append({'role': 'user', 'content': row['user_input']})\n",
        "        if row['assistant_response']:\n",
        "            chat_history.append({'role': 'assistant', 'content': row['assistant_response']})\n",
        "\n",
        "    # Add the current user input to the chat history\n",
        "    chat_history.append({'role': 'user', 'content': user_input})\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=chat_history,\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        gpt_response = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        gpt_response = f\"Error: {str(e)}\"\n",
        "\n",
        "    # Store new user input and assistant response in the database\n",
        "    table.insert(dict(user_input=user_input, assistant_response=gpt_response))\n",
        "\n",
        "    # Update and format the chat history for display\n",
        "    chat_history.append({'role': 'assistant', 'content': gpt_response})\n",
        "    formatted_chat_history = \"\\n\".join([f\"{msg['role'].title()}: {msg['content']}\" for msg in chat_history])\n",
        "\n",
        "    return formatted_chat_history\n",
        "\n",
        "your_interface = gr.Interface(\n",
        "    fn=my_gradio_function,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Your Message\"),\n",
        "    outputs=gr.Textbox(label=\"Chat History\"),\n",
        "    title=\"Chat with GPT-3.5\",\n",
        "    description=\"This is a simple chat app using Gradio and OpenAI's GPT-3.5, with chat history stored in SQLite via Dataset.\"\n",
        ")\n",
        "\n",
        "gr.TabbedInterface(\n",
        "    [your_interface], [\"NewGPTApp\"]\n",
        ").launch()"
      ],
      "metadata": {
        "id": "YvEiT5KfNBmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making an app to generate reports about Housing Violations\n",
        "\n",
        "One common use of AI is to tranform or contextualize data. The data on Housing Violations is available to the public on data.boston.gov:\n",
        "\n",
        "https://data.boston.gov/dataset/rentsmart\n",
        "\n",
        "## RENTSMART\n",
        "> RentSmart Boston compiles data from BOS:311 and the City's Inspectional Services Division to give prospective tenants a more complete picture of the homes and apartments they are considering renting, assisting them in understanding any previous issues with the property, including: housing violations, building violations, enforcement violations, housing complaints, sanitation requests, and/or civic maintenance requests.\n",
        "\n",
        "\n",
        "\n",
        "Let's make a gradio app that generates a report on building activities happening around a certain location. The parameters will be:\n",
        "1. model_name: the name of the model we are querying\n",
        "2. tokens: the number of tokens we are requesting\n",
        "3. latitude: latitude of the central location for the report\n",
        "3. longitude: longitude of the same\n",
        "5. radius: the distance for the radius of the circle within which to find permits\n",
        "6. language: the language of the report\n",
        "7. start-date: the earliest date for permits\n",
        "8. end-date: the latest date for permits\n",
        "\n"
      ],
      "metadata": {
        "id": "mOF2uq-tcPEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install haversine"
      ],
      "metadata": {
        "id": "10mwJt919Bj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all necessary imports\n",
        "import pandas as pd\n",
        "from pandas import to_datetime\n",
        "import pytz\n",
        "from haversine import haversine\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "import dataset\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import os\n",
        "\n",
        "csv_link = \"https://data.boston.gov/dataset/f506e000-b08c-4500-97c7-9f36e7ac125a/resource/dc615ff7-2ff3-416a-922b-f0f334f085d0/download/tmp2b5dwl3h.csv\"\n",
        "\n",
        "# Instantiate the OpenAI client\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))  # Replace with your API key\n",
        "\n",
        "\n",
        "def load_dataframe():\n",
        "    csv_file = \"data.csv\"\n",
        "    pkl_file = \"data.pkl\"\n",
        "\n",
        "    # Check if both files exist\n",
        "    if os.path.exists(csv_file) and os.path.exists(pkl_file):\n",
        "        # Compare modification times\n",
        "        csv_mod_time = os.path.getmtime(csv_file)\n",
        "        pkl_mod_time = os.path.getmtime(pkl_file)\n",
        "\n",
        "        if pkl_mod_time > csv_mod_time:\n",
        "            # Load from pickle if it's newer than csv\n",
        "            df = pd.read_pickle(pkl_file)\n",
        "            return df\n",
        "    # Check if the csv_file exists, if not, download it\n",
        "    if not os.path.exists(csv_file):\n",
        "        download_data(csv_link)\n",
        "\n",
        "\n",
        "    # If pickle is not newer, read csv and save as pickle\n",
        "    df = pd.read_csv(csv_file, dtype={'zipcode': str}, parse_dates=['date'])\n",
        "    df.to_pickle(pkl_file)\n",
        "    return df\n",
        "\n",
        "def download_data(url):\n",
        "    # Download data\n",
        "    response = requests.get(url)\n",
        "    file_path = 'data.csv'\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "\n",
        "def filter_dataset_by_location(df, lat, lon, radius):\n",
        "    filtered_df = {}\n",
        "    def is_within_radius(lat1, lon1, lat2, lon2, radius):\n",
        "        return haversine((lat1, lon1), (lat2, lon2)) <= radius\n",
        "            # Assuming columns 'lat' and 'long' exist and are correctly formatted\n",
        "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "        # Filter the DataFrame\n",
        "        mask = df.apply(lambda row: is_within_radius(lat, lon, row['latitude'], row['longitude'], radius), axis=1)\n",
        "        filtered_df = df[mask]\n",
        "    else:\n",
        "        print(f\"Latitude/Longitude columns not found in dataset\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "def filter_datasets_by_date(df, start_date, end_date):\n",
        "    if 'date' in df.columns:\n",
        "        mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
        "    else:\n",
        "        print(f\"No suitable date column found in {dataset_id}, returning original DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    filtered_df = df.loc[mask]\n",
        "    return filtered_df\n",
        "\n",
        "#function for Gradio to take all inputs from the user, load the df from the database, and then send it to OpenAI:\n",
        "def my_gradio_function(model_name, tokens, latitude, longitude, radius, start_date, end_date, language, prompt):\n",
        "    # Load the building permit dataframe\n",
        "    df = load_dataframe()\n",
        "\n",
        "    # Filter the permits by location\n",
        "    filtered_df_location = filter_dataset_by_location(df, latitude, longitude, radius)\n",
        "\n",
        "    # Convert the start and end dates to datetime objects:\n",
        "    start_dt = to_datetime(start_date).tz_localize(None)  # Example of making it timezone-naive\n",
        "    end_dt = to_datetime(end_date).tz_localize(None)\n",
        "\n",
        "    # Filter the permits by date\n",
        "    filtered_df_date = filter_datasets_by_date(filtered_df_location, start_dt, end_dt)\n",
        "\n",
        "    # Save the raw tabular data to a variable to be returned as an output in Gradio\n",
        "    raw_data_output = filtered_df_date.to_csv(index=False)\n",
        "\n",
        "    # Paste together the prompt, a request to give output in language, and the permits tabular data with newlines between\n",
        "    full_prompt = f\"{prompt}\\nLanguage: {language}\\nData:\\n{raw_data_output}\"\n",
        "\n",
        "    # Format as a messages object\n",
        "    full_prompt_messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=full_prompt_messages,\n",
        "            max_tokens=tokens,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        gpt_response = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        gpt_response = f\"Error: {str(e)}\"\n",
        "\n",
        "    # Return the response and the raw data to be displayed in separate Gradio outputs\n",
        "    return gpt_response, filtered_df_date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2zrql21I0LsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframe downloads the data, reads it into a pandas dataframe, and then saves that as a pkl for quick access\n",
        "df = load_dataframe()"
      ],
      "metadata": {
        "id": "OMAXNwR_OvJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our functions before we launch it in Gradio"
      ],
      "metadata": {
        "id": "WfJZ5p-6Eepq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_gpt_response, my_raw_data_output = my_gradio_function(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    tokens=100,\n",
        "    latitude=42.335817,\n",
        "    longitude=-71.0797682,\n",
        "    radius=0.25,\n",
        "    start_date=\"2024-01-01\",\n",
        "    end_date=\"2024-01-19\",\n",
        "    language=\"Spanish\",\n",
        "    prompt=f\"These records represent all violations found in a .25 km radius around a central point. summarize this history for a prospective renter of a unit at the center of this circle.\"\n",
        ")\n",
        "\n",
        "print(my_gpt_response)"
      ],
      "metadata": {
        "id": "nJxRGL6AEeTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_raw_data_output.head(20)"
      ],
      "metadata": {
        "id": "rxJOqs6WRi_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tIXwAdVlRiq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define default values for each input\n",
        "default_model = \"gpt-3.5-turbo\"\n",
        "default_tokens = 3000\n",
        "default_latitude = 42.335817\n",
        "default_longitude = -71.0797682\n",
        "default_radius = 0.25\n",
        "default_start_date = \"2024-01-01\"\n",
        "default_end_date = \"2024-01-19\"\n",
        "default_language = \"Spanish\"\n",
        "default_prompt = \"These records represent violations in a circle around a center point in Boston. Summarize them into a report for a prospective renter\"\n",
        "\n",
        "permit_interface = gr.Interface(\n",
        "    fn=my_gradio_function,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=[\"gpt-3.5-turbo-1106\",\"gpt-3.5-turbo\"], label=\"Model Name\", value=default_model),\n",
        "        gr.Number(label=\"# of Requested Tokens\", value=default_tokens),\n",
        "        gr.Number(label=\"Latitude\", value=default_latitude),\n",
        "        gr.Number(label=\"Longitude\", value=default_longitude),\n",
        "        gr.Number(label=\"Radius (in km)\", value=default_radius),\n",
        "        gr.Textbox(label=\"Start Date\", value=default_start_date),\n",
        "        gr.Textbox(label=\"End Date\", value=default_end_date),\n",
        "        gr.Dropdown(choices=[\"English\", \"Spanish\", \"French\", \"Arabic\", \"Chinese\", \"Vietnamese\", \"Japanese\", \"Korean\"], label=\"Language\", value=default_language),\n",
        "        gr.Textbox(label=\"Prompt\", value=default_prompt)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"GPT-3 Response\"),\n",
        "        gr.Dataframe(label=\"Filtered Data\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "gr.TabbedInterface(\n",
        "    [permit_interface], [\"TheBostonApp\"]\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "QkMXu29M19YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYlTCPNyTQSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}